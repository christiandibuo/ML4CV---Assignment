{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10068624,"sourceType":"datasetVersion","datasetId":6205623},{"sourceId":11194573,"sourceType":"datasetVersion","datasetId":6988526},{"sourceId":11472157,"sourceType":"datasetVersion","datasetId":6824487}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":22499.335479,"end_time":"2025-03-21T04:37:30.068045","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-20T22:22:30.732566","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"00fea8d42ca44a579538cb4de15921b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"055fc038486649be834d2536029b1172":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07843e3b529d40978c89e70467e6ef58":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b31995722143446896b7a970034bf061","placeholder":"​","style":"IPY_MODEL_b691782288664c5bad72d296f101a55f","tabbable":null,"tooltip":null,"value":" 25.6M/25.6M [00:00&lt;00:00, 146MB/s]"}},"18523a5c859c485fadfbe4457bf2c9ff":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a382a28885b464ab4cab89980857f13":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1a41bcf9b7b544f683c109bae9ef04a3":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"221efe3652574c7783c152b4006242eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_a35cc286f2d44f50bee86faf9d625ab4","placeholder":"​","style":"IPY_MODEL_258aef4a120b4c6696a669cc0ddcba4a","tabbable":null,"tooltip":null,"value":"preprocessor_config.json: 100%"}},"258aef4a120b4c6696a669cc0ddcba4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"287b6e0a717d4099bf6a93297284a9c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"4d6d10aaac834c7793576062ec4220a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"60a8b3d11828487a974b1ade6654dd4e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6360f3f0338c42629881ca05b4c648e9":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bac3a306686489e9ef89e62d56bf0b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_1a41bcf9b7b544f683c109bae9ef04a3","placeholder":"​","style":"IPY_MODEL_287b6e0a717d4099bf6a93297284a9c5","tabbable":null,"tooltip":null,"value":"config.json: 100%"}},"84d63490c5824f1bb14f0bcff14170e0":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1d75ea79bba4e3fa5a616965b0b9653":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a35cc286f2d44f50bee86faf9d625ab4":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad20226741a040848a66499c26db0266":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b31995722143446896b7a970034bf061":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5650599cd844c2684617c67a552b670":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b691782288664c5bad72d296f101a55f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"c2c67e7656e143c3a0f356fdc36052be":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_ad20226741a040848a66499c26db0266","placeholder":"​","style":"IPY_MODEL_f34da857958c41a299bb211ca18595ac","tabbable":null,"tooltip":null,"value":" 1.64k/1.64k [00:00&lt;00:00, 168kB/s]"}},"c6d08c7e905148edbaf6dd0e5e0f47e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_221efe3652574c7783c152b4006242eb","IPY_MODEL_e61c031c400b4ea1b3927d10ccfab879","IPY_MODEL_dd3ba3f35eb44716b1d794e3c7389e41"],"layout":"IPY_MODEL_cca46157f7fe48539a3babc7f5e9de9d","tabbable":null,"tooltip":null}},"c89ef6006e3045b0b87b5c02262752f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_055fc038486649be834d2536029b1172","max":25615631,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00fea8d42ca44a579538cb4de15921b6","tabbable":null,"tooltip":null,"value":25615631}},"cc53e025d33546df8e2fc2d9a3090001":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fbfa450ac67d4477a76590d6e2a521fc","IPY_MODEL_c89ef6006e3045b0b87b5c02262752f1","IPY_MODEL_07843e3b529d40978c89e70467e6ef58"],"layout":"IPY_MODEL_60a8b3d11828487a974b1ade6654dd4e","tabbable":null,"tooltip":null}},"cca46157f7fe48539a3babc7f5e9de9d":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d75f3adf8e854163b8a54dca0cd4a5fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"dd3ba3f35eb44716b1d794e3c7389e41":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fd9484f40f504d949b496e51061f137f","placeholder":"​","style":"IPY_MODEL_a1d75ea79bba4e3fa5a616965b0b9653","tabbable":null,"tooltip":null,"value":" 187/187 [00:00&lt;00:00, 18.7kB/s]"}},"e240be0f8e6341328f7632685004469a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7bac3a306686489e9ef89e62d56bf0b6","IPY_MODEL_f232409a7bf748abacdc460a7b3564c3","IPY_MODEL_c2c67e7656e143c3a0f356fdc36052be"],"layout":"IPY_MODEL_b5650599cd844c2684617c67a552b670","tabbable":null,"tooltip":null}},"e61c031c400b4ea1b3927d10ccfab879":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_6360f3f0338c42629881ca05b4c648e9","max":187,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1a382a28885b464ab4cab89980857f13","tabbable":null,"tooltip":null,"value":187}},"f232409a7bf748abacdc460a7b3564c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_84d63490c5824f1bb14f0bcff14170e0","max":1644,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d6d10aaac834c7793576062ec4220a4","tabbable":null,"tooltip":null,"value":1644}},"f34da857958c41a299bb211ca18595ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"fbfa450ac67d4477a76590d6e2a521fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_18523a5c859c485fadfbe4457bf2c9ff","placeholder":"​","style":"IPY_MODEL_d75f3adf8e854163b8a54dca0cd4a5fd","tabbable":null,"tooltip":null,"value":"pytorch_model.bin: 100%"}},"fd9484f40f504d949b496e51061f137f":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"### Author\n0001128790 - Christian Di Buò - christian.dibuo@studio.unibo.it","metadata":{"papermill":{"duration":0.010824,"end_time":"2025-03-20T22:22:33.411190","exception":false,"start_time":"2025-03-20T22:22:33.400366","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Importing images","metadata":{"papermill":{"duration":0.009766,"end_time":"2025-03-20T22:22:33.431084","exception":false,"start_time":"2025-03-20T22:22:33.421318","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os, json\nimport subprocess, sys\nimport random\nimport math\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport copy\nimport cv2\nfrom PIL import Image, ImageDraw\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom transformers import MobileViTImageProcessor, MobileViTForSemanticSegmentation","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":23.185363,"end_time":"2025-03-20T22:22:56.626381","exception":false,"start_time":"2025-03-20T22:22:33.441018","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:35:33.683264Z","iopub.execute_input":"2025-05-09T13:35:33.683481Z","iopub.status.idle":"2025-05-09T13:35:58.717327Z","shell.execute_reply.started":"2025-05-09T13:35:33.683463Z","shell.execute_reply":"2025-05-09T13:35:58.716699Z"}},"outputs":[{"name":"stderr","text":"2025-05-09 13:35:44.420055: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746797744.614492      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746797744.669158      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"package_name = \"evaluate\"\n\ntry:\n    __import__(package_name)\n    print('already installed')\nexcept ImportError:\n    print(f\"{package_name} is NOT installed! Installing now...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name]);","metadata":{"papermill":{"duration":4.629052,"end_time":"2025-03-20T22:23:01.265956","exception":false,"start_time":"2025-03-20T22:22:56.636904","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:35:58.717987Z","iopub.execute_input":"2025-05-09T13:35:58.718461Z"}},"outputs":[{"name":"stdout","text":"evaluate is NOT installed! Installing now...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import wandb\n\nWANDB_USER = \"chri-project\"  \nWANDB_PROJECT = \"ML4CV--assignment\"\nwandb.login(key='2b387b514b9fcec8902df2b863ae0646f56125d6')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install segmentation-models-pytorch albumentations --no-deps","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n#wget -c http://images.cocodataset.org/zips/train2017.zip -q\n#unzip -q train2017.zip\n#rm train2017.zip\n#mv train2017 coco_images_train\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\n\nwget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip -q\nunzip -q annotations_trainval2017.zip\nrm annotations_trainval2017.zip\nmv annotations coco_annotations\n\nwget -c http://images.cocodataset.org/zips/val2017.zip -q\nunzip -q val2017.zip \nrm val2017.zip\nmv val2017 coco_images_val","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fix_random(seed: int) -> None:\n    \"\"\"\n    Fix all the possible sources of randomness.\n\n    Args:\n        seed: the seed to use.\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nfix_random(seed=42)","metadata":{"papermill":{"duration":0.021698,"end_time":"2025-03-20T22:23:01.298593","exception":false,"start_time":"2025-03-20T22:23:01.276895","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate\n\n\"\"\"\nSource: https://github.com/hendrycks/anomaly-seg/issues/15#issuecomment-890300278\n\"\"\"\nCOLORS = np.array([\n    [  0,   0,   0],  # unlabeled    =   0,\n    [ 70,  70,  70],  # building     =   1,\n    [190, 153, 153],  # fence        =   2, \n    [250, 170, 160],  # other        =   3,\n    [220,  20,  60],  # pedestrian   =   4, \n    [153, 153, 153],  # pole         =   5,\n    [157, 234,  50],  # road line    =   6, \n    [128,  64, 128],  # road         =   7,\n    [244,  35, 232],  # sidewalk     =   8,\n    [107, 142,  35],  # vegetation   =   9, \n    [  0,   0, 142],  # car          =  10,\n    [102, 102, 156],  # wall         =  11, \n    [220, 220,   0],  # traffic sign =  12,\n    [ 60, 250, 240],  # anomaly      =  13,\n]) ","metadata":{"papermill":{"duration":0.924497,"end_time":"2025-03-20T22:23:02.233570","exception":false,"start_time":"2025-03-20T22:23:01.309073","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TODO: Show the imbalance of the classes, if any. In this way you can justify that there are errors\n## TODO: create cocodataset in which you order each image with its own annotation, in this way you can remove the for loop inside random_anomaly_injection function","metadata":{"execution":{"iopub.status.busy":"2025-04-23T14:06:25.487534Z","iopub.execute_input":"2025-04-23T14:06:25.487891Z","iopub.status.idle":"2025-04-23T14:06:25.491225Z","shell.execute_reply.started":"2025-04-23T14:06:25.487871Z","shell.execute_reply":"2025-04-23T14:06:25.490484Z"}}},{"cell_type":"code","source":"class StreetHazardsDataset(Dataset):\n    def __init__(self, odgt_file, coco_path_annotations = None, coco_path_images=None, image_resize=None, augment_both=None, augment_images=None, inject_anomalies = False):\n        \"\"\"\n        Args:\n            odgt_file (str): Path to the .odgt file (train, val, or test).\n            transform (callable, optional): Transformations to apply to images and masks.\n        \"\"\"\n\n        self.augment_both = augment_both\n        self.augment_images = augment_images\n        self.image_resize = image_resize\n        self.inject_anomalies = inject_anomalies\n        self.coco_path_images = coco_path_images\n        \n        if self.inject_anomalies:\n            with open(coco_path_annotations, 'r') as file:\n                    self.coco_data = json.load(file)\n\n        # Load the .odgt file\n        with open(odgt_file, \"r\") as f:\n            odgt_data = json.load(f)\n\n        self.paths = [\n            {\n                \"image\": os.path.join(Path(odgt_file).parent, data[\"fpath_img\"]),\n                \"labels\": os.path.join(Path(odgt_file).parent, data[\"fpath_segm\"]),\n            }\n            for data in odgt_data \n        ]\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, idx):\n\n        image = Image.open(self.paths[idx][\"image\"]).convert(\"RGB\")\n        labels = Image.open(self.paths[idx][\"labels\"])\n\n        if self.image_resize:\n            image = transforms.Resize(self.image_resize, transforms.InterpolationMode.BILINEAR)(image)\n            labels = transforms.Resize(self.image_resize, transforms.InterpolationMode.NEAREST)(labels)\n            \n        if self.augment_both:\n            image, labels = self.augment_both(image, labels)\n\n        if self.inject_anomalies:\n            \n            image, labels = random_anomaly_injection(image, labels, self.coco_data, self.coco_path_images)\n            \n\n        #to_tensor\n        image = transforms.ToTensor()(image)\n        labels = torch.as_tensor(transforms.functional.pil_to_tensor(labels), dtype=torch.int64) - 1\n        labels = labels.squeeze(0)\n        \n        if self.augment_images:\n            image = self.augment_images(image)\n\n        return {'image' : image, 'labels' : labels}","metadata":{"papermill":{"duration":0.021288,"end_time":"2025-03-20T22:23:02.266771","exception":false,"start_time":"2025-03-20T22:23:02.245483","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_annotation(annotation_img: np.ndarray|torch.Tensor, ax=None):\n    \"\"\"\n    Adapted from https://github.com/CVLAB-Unibo/ml4cv-assignment/blob/master/utils/visualize.py\n    \"\"\"\n    if ax is None: ax = plt.gca()\n    annotation_img = np.asarray(annotation_img)\n    img_new = np.zeros((*annotation_img.shape, 3))\n\n    for index, color in enumerate(COLORS):\n        img_new[annotation_img == index] = color\n\n    ax.imshow(img_new / 255.0)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    #wandb.log({\"examples\": [wandb.Image(img_new / 255.0)]})\n    \n    \n\ndef visualize_scene(image: np.ndarray|torch.Tensor, ax=None):\n    if ax is None: ax = plt.gca()\n    image = np.asarray(image)\n    ax.imshow(np.moveaxis(image, 0, -1))\n    ax.set_xticks([])\n    ax.set_yticks([])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''class CocoDataset(Dataset):\n    def __init__(self, annotations_path, images_path):\n        \"\"\"\n        Args:\n            odgt_file (str): Path to the .odgt file (train, val, or test).\n            transform (callable, optional): Transformations to apply to images and masks.\n        \"\"\"\n\n        self.images_path = images_path\n        \n        with open(annotations_path, \"r\") as f:\n            self.coco_data = json.load(f)\n\n        \n    def __len__(self):\n        return len(os.listdir(self.images_path))\n\n    def __getitem__(self, idx):\n\n        annotations = self.coco_data['annotations'][idx]\n        image_id = annotations['image_id']\n        \n        img_segmentation = annotations['segmentation'][0]\n        bbox = list(map(int, annotations['bbox']))\n        \n        for img in self.coco_data['images']:\n            if image_id == img['id']:\n                name = img['file_name']\n                im = cv2.imread(os.path.join(self.images_path, name))\n                \n                polygon = np.array([[int(img_segmentation[i]), int(img_segmentation[i + 1])] for i in range(0, len(img_segmentation), 2)])\n                mask = np.zeros((im.shape[0], im.shape[1]))\n                cv2.fillConvexPoly(mask, polygon, 1)\n        \n                mask = mask > 0\n                out = np.zeros_like(im)\n                out[mask] = im[mask]\n                x, y, w, h = bbox\n                image_out = out[y:y+h, x:x+w]\n                #plt.imshow(out)\n        \n                return image_out'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def random_anomaly_injection(hazard_image, hazard_label, coco_data, coco_image_path, overlay_scale_range = (0.1, 0.4)):\n    \n    # Load COCO annotation data\n    \n    idx = np.random.randint(0, 36781)\n    \n    annotations = coco_data['annotations'][idx]\n    '''categories = {\n        c['id']: {'supercategory': c['supercategory'], 'name': c['name']}\n        for c in coco_data['categories']\n    }\n    print(categories[annotations['category_id']])'''\n    \n    image_id = annotations['image_id']\n    \n    while (type(annotations['segmentation']) != list):\n        idx = np.random.randint(0, 36781)\n        annotations = coco_data['annotations'][idx]\n        image_id = annotations['image_id']\n    \n    img_segmentation = annotations['segmentation'][0]\n\n    hazard_img = np.array(hazard_image.copy())\n    hazard_lbl = np.array(hazard_label.copy())\n    hazard_height, hazard_width, _ = hazard_img.shape\n\n    #randomly compute the scale of the overlay\n    overlay_scale  = random.uniform(*overlay_scale_range)\n    overlay_size = (int(hazard_height*overlay_scale), int(hazard_width*overlay_scale))\n    \n    for img_info in coco_data['images']:\n        if image_id == img_info['id']:\n            name = img_info['file_name']\n            coco_img = cv2.imread(f\"{coco_image_path}/{name}\")\n            original_height, original_width = coco_img.shape[:2]\n    \n            # Create a binary mask from the segmentation polygon\n            polygon = np.array(img_segmentation).reshape((-1, 2)).astype(np.int32)\n            mask = np.zeros((original_height, original_width), dtype=np.uint8)\n            cv2.fillPoly(mask, [polygon], 255)\n    \n            # Crop object and mask\n            x, y, w, h = cv2.boundingRect(mask)\n            segmented_object = coco_img[y:y + h, x:x + w]\n            try:\n                segmented_object = cv2.cvtColor(segmented_object, cv2.COLOR_BGR2RGB)\n            except:\n                break\n            mask_cropped = mask[y:y + h, x:x + w]\n        \n            # Resize object and mask\n            if segmented_object.size > 0:\n                resized_object = cv2.resize(segmented_object, overlay_size, interpolation=cv2.INTER_LINEAR)\n                resized_mask = cv2.resize(mask_cropped, overlay_size, interpolation=cv2.INTER_NEAREST)\n                resized_mask_binary = resized_mask > 0\n    \n            # Random region in StreetHazards image to insert the anomaly\n            roi_x = np.random.randint(0, hazard_width - overlay_size[0])\n            roi_y = np.random.randint(0, hazard_height - overlay_size[1])\n            roi = hazard_img[roi_y:roi_y + overlay_size[1], roi_x:roi_x + overlay_size[0]].copy()\n    \n            # Create a mask with the same number of channels as the ROI\n            resized_mask_channels = np.expand_dims(resized_mask_binary, axis=-1).astype(float)\n            if roi.ndim == 3:\n                resized_mask_channels = np.repeat(resized_mask_channels, 3, axis=-1)\n            elif roi.ndim == 2:\n                resized_mask_channels = np.expand_dims(resized_mask_channels, axis=-1).astype(float) # For grayscale\n    \n            # Ensure resized_object has the same number of channels as roi\n            if resized_object.ndim == 2 and roi.ndim == 3:\n                resized_object_expanded = cv2.cvtColor(resized_object, cv2.COLOR_GRAY2BGR).astype(float) / 255.0\n            elif resized_object.ndim == 3:\n                resized_object = resized_object.astype(float) / 255.0\n            else:\n                print(\"Warning: Segmented object has incompatible dimensions for overlay.\")\n                continue\n    \n            roi = roi.astype(float) / 255.0\n    \n            # Blend the ROI with the resized segmented object using the mask\n            masked_roi = (roi * (1 - resized_mask_channels))\n            overlaid_part = (resized_object * resized_mask_channels)\n            blended_roi = cv2.addWeighted(masked_roi, 1, overlaid_part, 1, 0)\n            hazard_img[roi_y:roi_y + overlay_size[1], roi_x:roi_x + overlay_size[0]] = (blended_roi * 255).astype(np.uint8)\n\n            # Apply the anomaly also in the label\n            hazard_lbl[roi_y:roi_y + overlay_size[1], roi_x:roi_x + overlay_size[0]][resized_mask_binary] = 14\n\n            hazard_img, hazard_lbl = Image.fromarray(hazard_img), Image.fromarray(hazard_lbl)\n            return hazard_img, hazard_lbl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_mean_std(loader):\n    mean = 0.0\n    std = 0.0\n    nb_samples = 0\n\n    for batch in tqdm(loader):\n        images = batch[\"image\"]\n        batch_samples = images.size(0)\n        images = images.view(batch_samples, images.size(1), -1) \n    \n        mean += images.mean(2).sum(0)  \n        std += images.std(2).sum(0)\n        nb_samples += batch_samples \n        del batch\n\n    mean /= nb_samples\n    std /= nb_samples\n    return mean, std","metadata":{"papermill":{"duration":0.191359,"end_time":"2025-03-20T22:23:02.469377","exception":false,"start_time":"2025-03-20T22:23:02.278018","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_resize = (512, 896)\n\ntrain_dataset = StreetHazardsDataset(\n    odgt_file=\"/kaggle/input/ml4cv-data/streethazards_train/train/train.odgt\",\n    image_resize = image_resize,\n    augment_both=None,\n    augment_images=None,\n    inject_anomalies=False\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#compute mean and std on resized images should give better results, if the resize is the same used in the training\ntrain_dl = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\nmean_streethazards, std_streethazards = compute_mean_std(train_dl)\nprint(mean_streethazards, std_streethazards)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_imagenet, std_imagenet = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"augment_both = transforms.v2.Compose([\n    #transforms.v2.RandomCrop(image_resize_scale),\n    transforms.v2.RandomResizedCrop(image_resize, interpolation=transforms.InterpolationMode.NEAREST),\n    transforms.v2.RandomHorizontalFlip(),\n])\n\naugment_images = transforms.Compose([\n    transforms.Normalize(mean = mean_imagenet, std = std_imagenet),\n    #transforms.Normalize(mean = mean_streethazards, std = std_streethazards),\n    #transforms.RandomErasing(scale=(0.02, 0.15))\n])\n\n#only apply resize, to_tensor and normalization (computed on train)\naugment_val_test = transforms.Normalize(\n    #mean = mean_streethazards, std = std_streethazards\n    mean = mean_imagenet, std = std_imagenet\n)\n\n#Create dataset\ntrain_dataset = StreetHazardsDataset(\n    odgt_file=\"/kaggle/input/ml4cv-data/streethazards_train/train/train.odgt\",\n    image_resize = None,\n    augment_both=augment_both,\n    augment_images=augment_images,\n    inject_anomalies = True,\n    coco_path_annotations = \"/kaggle/working/coco_annotations/instances_val2017.json\",\n    coco_path_images = \"/kaggle/working/coco_images_val\",\n)\n\nval_dataset = StreetHazardsDataset(\n    odgt_file=\"/kaggle/input/ml4cv-data/streethazards_train/train/validation.odgt\",\n    image_resize = image_resize,\n    augment_both=None,\n    augment_images=augment_val_test,\n    inject_anomalies = False,\n    coco_path_annotations = \"/kaggle/working/coco_annotations/instances_val2017.json\",\n    coco_path_images = \"/kaggle/working/coco_images_val\",\n)\n\ntest_dataset = StreetHazardsDataset(\n    odgt_file=\"/kaggle/input/ml4cv-data/streethazards_test/test/test.odgt\",\n    image_resize = image_resize,\n    augment_both=None,\n    augment_images=augment_val_test,\n    inject_anomalies = False,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = 2\nimg, lbl = train_dataset[idx].values()\nvisualize_scene(img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_annotation(lbl)","metadata":{"papermill":{"duration":0.480028,"end_time":"2025-03-20T22:23:02.960215","exception":false,"start_time":"2025-03-20T22:23:02.480187","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### data loader creations","metadata":{}},{"cell_type":"code","source":"train_dl = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\nval_dl = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\ntest_dl = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def as_numpy(obj):\n    if torch.is_tensor(obj):\n        return obj.cpu().numpy()\n    else:\n        return np.array(obj)","metadata":{"papermill":{"duration":0.017645,"end_time":"2025-03-20T22:23:03.071689","exception":false,"start_time":"2025-03-20T22:23:03.054044","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoImageProcessor, SegformerForSemanticSegmentation\n\ndef get_model(model_name, num_classes, weights_path = None):\n\n    if \"segformer\" in model_name:\n        processor = AutoImageProcessor.from_pretrained(model_name)\n        model = SegformerForSemanticSegmentation.from_pretrained(model_name, num_labels=num_classes, ignore_mismatched_sizes=True, semantic_loss_ignore_index = 13)\n    elif \"deeplabv3\":\n        processor = MobileViTImageProcessor.from_pretrained(model_name)\n        model = MobileViTForSemanticSegmentation.from_pretrained(model_name, num_labels=num_classes, ignore_mismatched_sizes=True, semantic_loss_ignore_index = 13)\n    \n    if weights_path:\n        model.load_state_dict(torch.load(weights_path))\n        \n    model.to(DEVICE);\n    \n    return processor, model","metadata":{"papermill":{"duration":0.018238,"end_time":"2025-03-20T22:23:03.102181","exception":false,"start_time":"2025-03-20T22:23:03.083943","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MeanIoU:\n    \"\"\"\n    taken from https://github.com/Jun-CEN/Open-World-Semantic-Segmentation/blob/main/DeepLabV3Plus-Pytorch/metrics/stream_metrics.py\n    \"\"\"\n    def __init__(self, n_classes= 13):\n        self.n_classes = n_classes\n        self.confusion_matrix = np.zeros((n_classes, n_classes))\n        \n    def update(self, label_trues, logits):\n        label_preds = torch.argmax(logits, dim=1)\n        label_preds, label_trues = label_preds.cpu().numpy(), label_trues.cpu().numpy()\n        for lt, lp in zip(label_trues, label_preds):\n            self.confusion_matrix += self._fast_hist( lt.flatten(), lp.flatten())\n\n    def _fast_hist(self, label_true, label_pred):\n        mask = (label_true >= 0) & (label_true < self.n_classes)\n        hist = np.bincount(\n            self.n_classes * label_true[mask].astype(int) + label_pred[mask],\n            minlength=self.n_classes ** 2,\n        ).reshape(self.n_classes, self.n_classes)\n        return hist\n\n    def get_results(self):\n        \"\"\"Returns accuracy score evaluation result.\n            - overall accuracy\n            - mean accuracy\n            - mean IU\n            - fwavacc\n        \"\"\"\n        hist = self.confusion_matrix\n        acc = np.diag(hist).sum() / hist.sum()\n        acc_cls = np.diag(hist) / hist.sum(axis=1)\n        acc_cls = np.nanmean(acc_cls)\n        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n        mean_iu = np.nanmean(iu, axis= 0)\n        freq = hist.sum(axis=1) / hist.sum()\n        fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n        cls_iu = dict(zip(range(self.n_classes), iu))\n\n        return {\n                \"Overall Acc\": acc,\n                \"Mean Acc\": acc_cls,\n                \"FreqW Acc\": fwavacc,\n                \"Mean IoU\": mean_iu,\n                \"Class IoU\": cls_iu,\n            }\n","metadata":{"papermill":{"duration":0.021505,"end_time":"2025-03-20T22:23:03.135894","exception":false,"start_time":"2025-03-20T22:23:03.114389","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_measures(_pos, _neg):\n    pos = np.array(_pos[:]).reshape((-1, 1))\n    neg = np.array(_neg[:]).reshape((-1, 1))\n    examples = np.squeeze(np.vstack((pos, neg)))\n    labels = np.zeros(len(examples), dtype=np.int32)\n    labels[:len(pos)] += 1\n\n    auroc = metrics.roc_auc_score(labels, examples)\n    aupr = metrics.average_precision_score(labels, examples)\n\n    return auroc, aupr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn import metrics\n\ndef get_aupr(confs, seg_labels, out_label=13):\n\n    in_scores = confs[seg_labels != out_label]\n    out_scores = confs[seg_labels == out_label]\n    \n    if (len(out_scores) != 0) and (len(in_scores) != 0):\n        \n        auprs = []\n        measures = get_measures(in_scores, out_scores)\n        auprs.append(measures[1])\n        aupr = np.mean(auprs)\n\n    return aupr","metadata":{"papermill":{"duration":0.020094,"end_time":"2025-03-20T22:23:03.168282","exception":false,"start_time":"2025-03-20T22:23:03.148188","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kornia.morphology import dilation, erosion\nfrom scipy import ndimage as ndi\n\nd_k1 = torch.zeros((1, 1, 2 * 1 + 1, 2 * 1 + 1)).cuda()\nd_k2 = torch.zeros((1, 1, 2 * 2 + 1, 2 * 2 + 1)).cuda()\nd_k3 = torch.zeros((1, 1, 2 * 3 + 1, 2 * 3 + 1)).cuda()\nd_k4 = torch.zeros((1, 1, 2 * 4 + 1, 2 * 4 + 1)).cuda()\nd_k5 = torch.zeros((1, 1, 2 * 5 + 1, 2 * 5 + 1)).cuda()\nd_k6 = torch.zeros((1, 1, 2 * 6 + 1, 2 * 6 + 1)).cuda()\nd_k7 = torch.zeros((1, 1, 2 * 7 + 1, 2 * 7 + 1)).cuda()\nd_k8 = torch.zeros((1, 1, 2 * 8 + 1, 2 * 8 + 1)).cuda()\nd_k9 = torch.zeros((1, 1, 2 * 9 + 1, 2 * 9 + 1)).cuda()\n\nd_ks = {1: d_k1, 2: d_k2, 3: d_k3, 4: d_k4, 5: d_k5, 6: d_k6, 7: d_k7, 8: d_k8, 9: d_k9}\n\n\nselem = torch.ones((3, 3)).cuda()\nselem_dilation = torch.FloatTensor(ndi.generate_binary_structure(2, 1)).cuda()\n\nfor k, v in d_ks.items():\n    v[:,:,k,k] = 1\n    for i in range(k):\n        v = dilation(v, selem_dilation)\n    d_ks[k] = v.squeeze(0).squeeze(0)\n\ndef find_boundaries(labels):\n    \"\"\"\n    Calculate boundary mask by getting diff of dilated and eroded prediction maps\n    \"\"\"\n    assert len(labels.shape) == 4\n    boundaries = (dilation(labels.float(), selem_dilation) != erosion(labels.float(), selem)).float()\n    ### save_image(boundaries, f'boundaries_{boundaries.float().mean():.2f}.png', normalize=True)\n\n    return boundaries\n\ndef expand_boundaries(boundaries, r=0):\n    \"\"\"\n    Expand boundary maps with the rate of r\n    \"\"\"\n    if r == 0:\n        return boundaries\n    expanded_boundaries = dilation(boundaries, d_ks[r])\n    ### save_image(expanded_boundaries, f'expanded_boundaries_{r}_{boundaries.float().mean():.2f}.png', normalize=True)\n    return expanded_boundaries","metadata":{"papermill":{"duration":1.379657,"end_time":"2025-03-20T22:23:04.560336","exception":false,"start_time":"2025-03-20T22:23:03.180679","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BoundarySuppressionWithSmoothing(nn.Module):\n    \"\"\"\n    Apply boundary suppression and dilated smoothing\n    \"\"\"\n    def __init__(self, boundary_suppression=True, boundary_width=4, boundary_iteration=4,\n                 dilated_smoothing=True, kernel_size=7, dilation=6):\n        super(BoundarySuppressionWithSmoothing, self).__init__()\n        self.kernel_size = kernel_size\n        self.dilation = dilation\n        self.boundary_suppression = boundary_suppression\n        self.boundary_width = boundary_width\n        self.boundary_iteration = boundary_iteration\n\n        sigma = 1.0\n        size = 7\n        gaussian_kernel = np.fromfunction(lambda x, y: (1/(2*math.pi*sigma**2)) * math.e ** ((-1*((x-(size-1)/2)**2+(y-(size-1)/2)**2))/(2*sigma**2)), (size, size))\n        gaussian_kernel /= np.sum(gaussian_kernel)\n        gaussian_kernel = torch.Tensor(gaussian_kernel).unsqueeze(0).unsqueeze(0)\n        self.dilated_smoothing = dilated_smoothing\n\n        self.first_conv = nn.Conv2d(1, 1, kernel_size=3, stride=1, bias=False)\n        self.first_conv.weight = torch.nn.Parameter(torch.ones_like((self.first_conv.weight)))\n\n        self.second_conv = nn.Conv2d(1, 1, kernel_size=self.kernel_size, stride=1, dilation=self.dilation, bias=False)\n        self.second_conv.weight = torch.nn.Parameter(gaussian_kernel)\n\n\n    def forward(self, x, prediction=None):\n        if len(x.shape) == 3:\n            x = x.unsqueeze(1)\n        x_size = x.size()\n        # B x 1 x H x W\n        assert len(x.shape) == 4\n        out = x\n        if self.boundary_suppression:\n            # obtain the boundary map of width 2 by default\n            # this can be calculated by the difference of dilation and erosion\n            boundaries = find_boundaries(prediction.unsqueeze(1))\n            expanded_boundaries = None\n            if self.boundary_iteration != 0:\n                assert self.boundary_width % self.boundary_iteration == 0\n                diff = self.boundary_width // self.boundary_iteration\n            for iteration in range(self.boundary_iteration):\n                if len(out.shape) != 4:\n                    out = out.unsqueeze(1)\n                prev_out = out\n                # if it is the last iteration or boundary width is zero\n                if self.boundary_width == 0 or iteration == self.boundary_iteration - 1:\n                    expansion_width = 0\n                # reduce the expansion width for each iteration\n                else:\n                    expansion_width = self.boundary_width - diff * iteration - 1\n                # expand the boundary obtained from the prediction (width of 2) by expansion rate\n                expanded_boundaries = expand_boundaries(boundaries, r=expansion_width)\n                # invert it so that we can obtain non-boundary mask\n                non_boundary_mask = 1. * (expanded_boundaries == 0)\n\n                f_size = 1\n                num_pad = f_size\n\n                # making boundary regions to 0\n                x_masked = out * non_boundary_mask\n                x_padded = nn.ReplicationPad2d(num_pad)(x_masked)\n\n                non_boundary_mask_padded = nn.ReplicationPad2d(num_pad)(non_boundary_mask)\n\n                # sum up the values in the receptive field\n                y = self.first_conv(x_padded)\n                # count non-boundary elements in the receptive field\n                num_calced_elements = self.first_conv(non_boundary_mask_padded)\n                num_calced_elements = num_calced_elements.long()\n\n                # take an average by dividing y by count\n                # if there is no non-boundary element in the receptive field,\n                # keep the original value\n                avg_y = torch.where((num_calced_elements == 0), prev_out, y / num_calced_elements)\n                out = avg_y\n\n                # update boundaries only\n                out = torch.where((non_boundary_mask == 0), out, prev_out)\n                del expanded_boundaries, non_boundary_mask\n\n            # second stage; apply dilated smoothing\n            if self.dilated_smoothing == True:\n                out = nn.ReplicationPad2d(self.dilation * 3)(out)\n                out = self.second_conv(out)\n\n            return out.squeeze(1)\n        else:\n            if self.dilated_smoothing == True:\n                out = nn.ReplicationPad2d(self.dilation * 3)(out)\n                out = self.second_conv(out)\n            else:\n                out = x\n\n        return out.squeeze(1)\n","metadata":{"papermill":{"duration":0.025793,"end_time":"2025-03-20T22:23:04.598205","exception":false,"start_time":"2025-03-20T22:23:04.572412","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_anomaly_score(score, mode='energy'):\n    score = score.squeeze(0)[:13]\n    if mode == 'energy':\n        anomaly_score = -(1. * torch.logsumexp(score, dim=1))\n    elif mode == 'entropy':\n        prob = torch.softmax(score, dim=0)\n        anomaly_score = -torch.sum(prob * torch.log(prob), dim=1) / torch.log(torch.tensor(19.))\n    else:\n        raise NotImplementedError\n\n    # regular gaussian smoothing\n    anomaly_score = anomaly_score.unsqueeze(0)\n    anomaly_score = transforms.GaussianBlur(7, sigma=1)(anomaly_score)\n    anomaly_score = anomaly_score.squeeze(0)\n    return anomaly_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import Tensor, einsum\n\ndef simplex(t: Tensor, axis=1) -> bool:\n    \"\"\"\n    taken from https://github.com/LIVIAETS/boundary-loss/blob/master/utils.py\n    \"\"\"\n    _sum = cast(Tensor, t.sum(axis).type(torch.float32))\n    _ones = torch.ones_like(_sum, dtype=torch.float32)\n    return torch.allclose(_sum, _ones)\n\nclass GeneralizedDice():\n    \"\"\"\n    taken from https://github.com/LIVIAETS/boundary-loss/blob/master/losses.py\n    \"\"\"\n    def __init__(self, **kwargs):\n        # Self.idc is used to filter out some classes of the target mask. Use fancy indexing\n        self.idc: List[int] = kwargs[\"idc\"]\n        print(f\"Initialized {self.__class__.__name__} with {kwargs}\")\n\n    def __call__(self, probs: Tensor, target: Tensor) -> Tensor:\n        assert simplex(probs) and simplex(target)\n\n        pc = probs[:, self.idc, ...].type(torch.float32)\n        tc = target[:, self.idc, ...].type(torch.float32)\n\n        w: Tensor = 1 / ((einsum(\"bkwh->bk\", tc).type(torch.float32) + 1e-10) ** 2)\n        intersection: Tensor = w * einsum(\"bkwh,bkwh->bk\", pc, tc)\n        union: Tensor = w * (einsum(\"bkwh->bk\", pc) + einsum(\"bkwh->bk\", tc))\n\n        divided: Tensor = 1 - 2 * (einsum(\"bk->b\", intersection) + 1e-10) / (einsum(\"bk->b\", union) + 1e-10)\n\n        loss = divided.mean()\n\n        return loss\n\nclass BoundaryLoss():\n    \"\"\"\n    taken from https://github.com/LIVIAETS/boundary-loss/blob/master/losses.py\n    \"\"\"\n    def __init__(self, **kwargs):\n        # Self.idc is used to filter out some classes of the target mask. Use fancy indexing\n        self.idc: List[int] = kwargs[\"idc\"]\n        print(f\"Initialized {self.__class__.__name__} with {kwargs}\")\n\n    def __call__(self, probs: Tensor, dist_maps: Tensor) -> Tensor:\n        assert simplex(probs)\n        assert not one_hot(dist_maps)\n\n        pc = probs[:, self.idc, ...].type(torch.float32)\n        dc = dist_maps[:, self.idc, ...].type(torch.float32)\n\n        multipled = einsum(\"bkwh,bkwh->bkwh\", pc, dc)\n\n        loss = multipled.mean()\n\n        return loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Trainer:\n    def __init__(self,\n                 #processor,\n                 model: nn.Module,\n                 train_loader: DataLoader,\n                 val_loader: DataLoader,\n                 device: torch.device,\n                 num_classes: int,\n                 loss,\n                 cfg: dict,\n                 model_name: str,\n                 resume_ckpt: dict = None,\n                 \n        ) -> None:\n        \n        self.model_name = model_name\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.num_classes = num_classes\n        self.patience = cfg[\"patience\"]\n        self.multi_scale = BoundarySuppressionWithSmoothing()\n        self.multi_scale.to(DEVICE)\n        self.loss = loss\n        #self.processor = processor\n        \n        if resume_ckpt:\n\n            self.model = model.to(device)\n            self.model.load_state_dict(resume_ckpt['model_state_dict'])\n            \n            self.num_epochs = cfg[\"num_epochs\"] - resume_ckpt['epoch']\n            num_steps = self.num_epochs * len(train_loader)\n            \n            self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"wd\"])\n            self.optimizer.load_state_dict(resume_ckpt['optimizer_state_dict'])\n            \n            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, cfg[\"lr\"], total_steps=num_steps)\n            self.scheduler.load_state_dict(resume_ckpt['scheduler_state_dict'])\n            \n\n\n        else:\n            self.model = model.to(device)\n            self.num_epochs = cfg[\"num_epochs\"]\n            num_steps = self.num_epochs * len(train_loader)\n            self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"wd\"])\n            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, cfg[\"lr\"], total_steps=num_steps)\n\n        self.mean_iou = 0.0\n        self.step = 0\n        self.best_acc = 0.0\n\n        self.ckpt_path = Path(\"ckpts\")\n        self.ckpt_path.mkdir(exist_ok=True)\n\n        wandb.init(name=cfg[\"run_name\"], entity=WANDB_USER, project=WANDB_PROJECT, config=cfg)\n\n\n    def train(self, verbose= False) -> None:\n        for epoch in tqdm(range(self.num_epochs), desc=\"Epoch\"):\n            \n            self.model.train()\n\n            for batch in self.train_loader:\n                imgs = batch['image'].to(self.device)\n                labels = batch['labels'].to(self.device)\n\n                if self.loss == energy_loss:\n\n                    vanilla_logits, mix_logits = self.model(imgs)\n                    loss_dict = self.loss(logits=mix_logits, targets=labels.clone(),\n                                         vanilla_logits=vanilla_logits)\n                    inlier_loss = loss_dict[\"entropy_part\"] + loss_dict[\"reg\"]\n                    outlier_loss = loss_dict[\"energy_part\"] * 0.05 # 0.05 = energy_weight (taken from paper)\n                    loss_res = inlier_loss + outlier_loss\n\n                else:\n\n                    logits = self.model(imgs)\n                    loss_res = self.loss(logits, labels)\n\n                self.optimizer.zero_grad()\n                loss_res.backward()\n                self.optimizer.step()\n                self.scheduler.step()\n\n            self.eval(\"train\", epoch)\n            self.eval(\"val\", epoch)\n\n            if self.patience < self.step:\n                break\n\n\n    @torch.no_grad()\n    def eval(self, split: str, epoch: int) -> None:\n        \n        self.model.eval()\n\n        loader = self.train_loader if split == \"train\" else self.val_loader\n        \n        mean_iou = MeanIoU()\n        losses = []\n        mean_avg = []\n        std_avg = []\n        \n        for batch in loader:\n            imgs = batch['image'].to(self.device)\n            labels = batch['labels'].to(self.device)\n            \n            if self.loss == energy_loss:\n                vanilla_logits, mix_logits = self.model(imgs)\n                loss_dict = self.loss(logits=mix_logits, targets=labels.clone(),\n                                     vanilla_logits=vanilla_logits)\n                inlier_loss = loss_dict[\"entropy_part\"] + loss_dict[\"reg\"]\n                outlier_loss = loss_dict[\"energy_part\"] * 0.05\n                loss_res = inlier_loss + outlier_loss\n\n            else:\n                vanilla_logits = self.model(imgs)\n                loss_res = self.loss(vanilla_logits, labels)\n            \n            losses.append(loss_res.item())\n\n            mean_iou.update(labels, vanilla_logits)\n        \n        results = mean_iou.get_results()\n        mean_iou = results['Mean IoU']\n        \n        l = sum(losses) / len(losses)\n        if split == \"val\":\n            print(f\"Epoch {epoch + 1} | {split.upper()} Metrics:\")\n            print(f\"  Loss: {l:.4f}\")\n            print(f\"  Mean IoU: {mean_iou:.4f}\\n\")\n\n            wandb.log({\n            \"val_loss\": l,\n            \"mean_iou\": mean_iou,\n            }, step=(epoch + 1))\n\n        if mean_iou > self.mean_iou and split == \"val\":\n            self.mean_iou = mean_iou\n            torch.save(self.model.state_dict(), self.ckpt_path/f\"{self.model_name}.pt\")\n            torch.save({\n                'epoch': epoch,\n                'mean_iou': self.mean_iou,\n                #'loss': loss,\n                'model_state_dict': self.model.state_dict(),\n                'optimizer_state_dict': self.optimizer.state_dict(),\n                'scheduler_state_dict': self.scheduler.state_dict(),\n                }, self.ckpt_path / \"best_checkpoint\")\n\n            wandb.save(self.ckpt_path/f\"{self.model_name}.pt\")\n            wandb.save(self.ckpt_path / \"best_checkpoint\")\n            \n            self.best_model = copy.deepcopy(self.model)\n            self.step = 0\n\n        elif split == \"val\":\n            self.step += 1","metadata":{"papermill":{"duration":0.029592,"end_time":"2025-03-20T22:23:04.639603","exception":false,"start_time":"2025-03-20T22:23:04.610011","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef predict(model, loader, verbose= False):\n\n    model.eval()\n    mean_iou = MeanIoU()\n    aupr = []\n\n    mean_logit = []\n    std_logit = []\n    #anomaly_score_list = []\n    labels_list = []\n    \n    mean_aupr = 0\n    auprs_count = 0\n    for batch in tqdm(loader):\n        \n        imgs = batch['image'].to(DEVICE)\n        labels = batch['labels'].to(DEVICE)\n\n        pred = model(imgs)\n        \n        if type(pred) == tuple:\n            _, logits = pred\n        else:\n            logits = pred\n            \n        #labels_list.append(labels)\n        \n        mean_iou.update(label_trues=labels, logits= logits)\n\n        #rpl anomaly\n        anomaly_score = compute_anomaly_score(logits, mode=\"energy\").cpu()\n\n        '''mean, std = get_mean_std(logits)\n        mean_logit.append(mean)\n        std_logit.append(std)\n        return np.mean(mean_logit, axis = 0), np.mean(std_logit, axis = 0)'''\n\n        \"\"\"\n        predictors\n        \"\"\"\n\n        #anomaly_score = maximum_softmax_probability(logits).cpu()#, multi_scale).cpu()\n        #anomaly_score = max_logit(logits).cpu()#, multi_scale).cpu()\n        #anomaly_score = euclidean_distance_sum(logits).cpu()\n        \n        '''anomaly_score = standardized_max_logit(\n                                      logits, multi_scale,\n                                      class_mean = MEAN_PER_CLASS, \n                                      class_var = VAR_PER_CLASS).cpu()'''\n        for i in range(len(anomaly_score)):\n            aupr, ac = compute_aupr(anomaly_score[i], (labels[i] ==13))\n            auprs_count += ac\n            mean_aupr += aupr\n        \n        del imgs\n        del labels\n        \n        #anomaly_score = anomaly_score.squeeze(0)\n        #anomaly_score_list.append(anomaly_score)\n            \n        \n        #conf = as_numpy(conf.squeeze(0).cpu())\n    mean_aupr = mean_aupr/auprs_count*100\n    #aupr = get_aupr(anomaly_score_list, labels_list)\n    return {\"aupr\": mean_aupr, \"mean_iou\": mean_iou.get_results()}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_aupr(preds, labels):\n    aupr_accumulator = 0\n    auprs_count = 0\n    if preds.dim() == 2: preds = preds.unsqueeze(0)\n    if labels.dim() == 2: labels = labels.unsqueeze(0)\n    preds, labels = preds.cpu(), labels.cpu()\n    for p, l in zip(preds, labels):\n        aupr_accumulator += metrics.average_precision_score(l.type(torch.int32).flatten().numpy(), p.type(torch.float32).flatten().numpy())\n        auprs_count += 1\n\n    return aupr_accumulator, auprs_count","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_mean_std(logits, num_classes = 13):\n\n    mean_avg = np.zeros(13)\n    std_avg = np.zeros(13)\n    \n    for logit in logits:\n        conf, labels = torch.max(logit, 0)\n        for c in range(num_classes):\n            tens = torch.where(labels == c, conf, 0)\n            mean, std = torch.std_mean(tens)\n\n            mean_avg[c] += as_numpy(mean)\n            std_avg[c] += as_numpy(std)\n\n    return mean_avg, std_avg**2","metadata":{"papermill":{"duration":0.017156,"end_time":"2025-03-20T22:23:04.668532","exception":false,"start_time":"2025-03-20T22:23:04.651376","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def maximum_softmax_probability(logits, multi_scale = None):\n    \"\"\"\n    taken from https://github.com/Jun-CEN/Open-World-Semantic-Segmentation/blob/main/anomaly/eval_ood_traditional.py#L185\n    \"\"\"\n    conf, prediction = torch.max(nn.functional.softmax(logits, dim=1),dim=1)\n    #taken from standardized max logit\n    if multi_scale:\n        with torch.no_grad():\n            conf = multi_scale(conf, prediction)\n    \n    return 1-conf","metadata":{"papermill":{"duration":0.018279,"end_time":"2025-03-20T22:23:04.698550","exception":false,"start_time":"2025-03-20T22:23:04.680271","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def max_logit(logits, multi_scale = None):\n    \"\"\"\n    taken from https://github.com/Jun-CEN/Open-World-Semantic-Segmentation/blob/main/anomaly/eval_ood_traditional.py#L185\n    \"\"\"\n    conf, prediction  = torch.max(logits,dim=1)\n    \n    #taken from standardized max logit\n    if multi_scale:\n        with torch.no_grad():\n            conf = multi_scale(conf, prediction)\n\n    return conf","metadata":{"papermill":{"duration":0.017608,"end_time":"2025-03-20T22:23:04.728418","exception":false,"start_time":"2025-03-20T22:23:04.710810","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MEAN_PER_CLASS = [1.72267947e+01, 7.66881425e+00, 5.77455433e-01, 4.47677614e-02,\n 0.00000000e+00, 3.41044600e-01, 1.39990459e+00, 2.11263614e+01,\n 7.31018189e+00, 7.46783295e+00, 1.15309967e-02, 2.41669891e+00,\n 0.00000000e+00]\nVAR_PER_CLASS = [1.33243940e+02, 1.53900278e+01, 2.14974713e-02, 3.87089461e-05,\n 0.00000000e+00, 7.03953145e-03, 8.06856298e-02, 2.19127594e+02,\n 5.62604913e+00, 8.48272837e+00, 1.52454267e-05, 8.70028476e-01,\n 0.00000000e+00]\n\ndef standardized_max_logit(logits, multi_scale, class_mean, class_var, num_classes = 13):\n\n    conf, prediction = torch.max(logits,dim=1)\n    for c in range(num_classes):\n        conf = torch.where(\n            prediction == c,\n            (conf - class_mean[c]) / np.sqrt(class_var[c]),\n            conf)\n\n    if multi_scale:\n        with torch.no_grad():\n            conf = multi_scale(conf, prediction)\n\n    return conf","metadata":{"papermill":{"duration":0.017984,"end_time":"2025-03-20T22:23:04.758280","exception":false,"start_time":"2025-03-20T22:23:04.740296","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def euclidean_distance_sum(logits):\n    \n    \"\"\"\n    taken from https://github.com/Jun-CEN/Open-World-Semantic-Segmentation/blob/main/anomaly/eval_ood_traditional.py#L185\n    \"\"\"\n\n    def Normalization(x):\n        return (x - np.min(x)) / (np.max(x) - np.min(x))\n\n    def Coefficient_map(x, thre):\n        lamda = 50\n        return 1 / (1 + np.exp(lamda * (x - thre)))\n        \n    dis_sum = torch.sum(logits,dim=1)\n    dis_sum = - as_numpy(dis_sum.squeeze(0).cpu())\n    dis_sum[dis_sum >= 400] = 400\n    dis_sum = Normalization(dis_sum)\n    prob_map = np.max(nn.functional.softmax(logits, dim=1).squeeze().cpu().numpy(), axis=1)\n    prob_map = Normalization(prob_map)\n    Coefficient = Coefficient_map(dis_sum, 0.2)\n    conf = Coefficient * dis_sum + (1 - Coefficient) * prob_map\n    conf = dis_sum\n\n    return conf","metadata":{"papermill":{"duration":0.019075,"end_time":"2025-03-20T22:23:04.789135","exception":false,"start_time":"2025-03-20T22:23:04.770060","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### mm-opensegmentation lab","metadata":{}},{"cell_type":"markdown","source":"### get the model\n\nonly specify the weights path if you do not want to train the model","metadata":{"papermill":{"duration":0.011884,"end_time":"2025-03-20T22:23:04.813141","exception":false,"start_time":"2025-03-20T22:23:04.801257","status":"completed"},"tags":[]}},{"cell_type":"code","source":"'''#model_name = \"apple/deeplabv3-mobilevit-small\"\n#model_name = \"nvidia/segformer-b1-finetuned-cityscapes-1024-1024\" #results 0.61 on validation, but i have used 512, 512 img size, 1024, 1024 too big\nmodel_name = \"nvidia/segformer-b4-finetuned-ade-512-512\"\n#model_name = \"nvidia/segformer-b2-finetuned-ade-512-512\" #no-augmentation: (0,633 on val_set)\n\n\n#weights_path = \"/kaggle/input/weights/segformer_23_mln.pt\"\nweights_path = None\n\n#processor, model = get_model(model_name, num_classes = len(COLORS)-1, weights_path = weights_path)\n'''","metadata":{"papermill":{"duration":1.789199,"end_time":"2025-03-20T22:23:06.614716","exception":false,"start_time":"2025-03-20T22:23:04.825517","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import segmentation_models_pytorch as smp\nfrom torchinfo import summary\n\nENCODER = 'efficientnet-b0'\nENCODER_WEIGHTS = 'imagenet'\nACTIVATION = None # could be None for logits or 'softmax2d' for multiclass segmentation\n\ndef get_deeplab_model(encoder_name=\"efficientnet-b0\", encoder_weights = \"imagenet\", activation= None, num_classes= 13):\n\n    # create segmentation model with pretrained encoder\n    model = smp.DeepLabV3Plus(\n        encoder_name=encoder_name, \n        encoder_weights=encoder_weights, \n        classes=num_classes,\n        activation=activation,\n    )\n\n    return model\n\nmodel = get_deeplab_model(encoder_name=\"efficientnet-b0\", encoder_weights = \"imagenet\", activation= None, num_classes= 13)\ninput_size = (8, 3, 512, 512)\nsummary = summary(model, input_size=input_size)\nprint(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"model trainable parameters before freezing are {trainable_params/1000000} milions\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''for name, param in model.backbone.named_parameters():\n    param.requires_grad = False\nfor name, param in model.backbone.layer4.named_parameters():\n    param.requires_grad = True\nfor name, param in model.classifier.named_parameters():\n    param.requires_grad = True\nfor name, param in model.aux_classifier.named_parameters():\n    param.requires_grad = True\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"model trainable parameters are {trainable_params/1000000} milions\")'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### model training","metadata":{"papermill":{"duration":0.011911,"end_time":"2025-03-20T22:23:06.672625","exception":false,"start_time":"2025-03-20T22:23:06.660714","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model_name = \"deeplabv3plus_efficientnet_b0\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cfg = {\n    \"num_epochs\" : 200,\n    \"lr\": 2e-4,\n    \"wd\": 0.001,\n    \"patience\": 1000,\n    \"loss\" : \"CrossEntropyLoss\",\n    \"optimizer\" : \"AdamW\",\n    \"data_augmentation\" : \"transforms.v2.RandomCrop(image_resize_scale),transforms.v2.RandomHorizontalFlip(), transforms.v2.Resize(image_resize_scale, transforms.InterpolationMode.NEAREST)\",\n    \"validation\" : \"not augmented with anomalies\",\n    \"run_name\": model_name,\n}","metadata":{"papermill":{"duration":0.017997,"end_time":"2025-03-20T22:23:06.702745","exception":false,"start_time":"2025-03-20T22:23:06.684748","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def resume_run(run_id):\n\n    api = wandb.Api()\n    run = api.run(run_id)\n\n    ckpts_files = []\n    files = run.files()\n    for f in files:\n        if f.name.startswith(\"ckpts/\"):\n            f.download(replace=True)\n            ckpts_files.append(f\"/kaggle/working/{f.name}\")\n    return ckpts_files","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run_id = \"chri-project/ML4CV--assignment/1udth88b\"\nresume_ckpt = resume_run(run_id)\nmodel_weigths = torch.load(resume_ckpt[1], weights_only=True)\nmodel.load_state_dict(model_weigths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''trainer = Trainer(\n    #processor = processor,\n    model= model,\n    train_loader= train_dl,\n    val_loader= val_dl ,\n    loss = F.cross_entropy,\n    device= DEVICE,\n    num_classes = len(COLORS)-1,\n    model_name = model_name,\n    cfg= cfg,\n    resume_ckpt = torch.load(resume_ckpt[0])\n)'''\n\n#trainer.train()","metadata":{"papermill":{"duration":0.026267,"end_time":"2025-03-20T22:23:06.741441","exception":false,"start_time":"2025-03-20T22:23:06.715174","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_train = predict(model, train_dl)\nprint(\"train results: \", result_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_val = predict(model, val_dl)\nprint(\"validation results: \", result_val)","metadata":{"papermill":{"duration":55.308409,"end_time":"2025-03-21T04:37:26.872763","exception":false,"start_time":"2025-03-21T04:36:31.564354","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_test = predict(model, test_dl)\nprint(\"test_results: \", result_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef get_predictions(img, model):\n    img = img.unsqueeze(0).to(DEVICE)\n    pred = model(img)\n    log = pred\n    log = log.cpu()\n    log = torch.argmax(log, dim=1)\n    log = log.squeeze(0)\n    return log","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = val_dataset\nfig, ax = plt.subplots(1, 2, figsize=(10, 12))\n\nidx = 0\nimg, lbl = dataset[idx].values()\nvisualize_annotation(get_predictions(img, model), ax[0])\nvisualize_annotation(lbl, ax[1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''idx = 500\ndataset = train_dataset\nfig, ax = plt.subplots(1, 2)\n\ntr_img, tr_lbl = dataset[idx].values()\nvisualize_annotation(get_predictions(tr_img, model), ax[0])\nvisualize_annotation(tr_lbl, ax[1])\n\n#wandb.finish()'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### AUPR Results:\n- standardized max logit is the worst performer: 6.11 (if use boundiary suppression it goes up to 7.sth)\n- max logit: 7.65\n- maximum softmax probability: 11.33\n- euclidean distance: 15.31","metadata":{"papermill":{"duration":0.030742,"end_time":"2025-03-21T04:37:26.936022","exception":false,"start_time":"2025-03-21T04:37:26.905280","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### TODO: implement class AUPR","metadata":{"papermill":{"duration":0.030487,"end_time":"2025-03-21T04:37:27.134215","exception":false,"start_time":"2025-03-21T04:37:27.103728","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# RPL implementation","metadata":{}},{"cell_type":"markdown","source":"make arrays:\n- in which you insert all the layer up to the layer2 (representation extracted from shallow layers to go through conv2d)\n- one in which you put everything up to the end of resnet (going through both ASPP and ResidualBlock)\n- another one for ASPP (concatenate then with the first)","metadata":{}},{"cell_type":"code","source":"def disimilarity_entropy(logits, vanilla_logits, t=1.):\n    n_prob = torch.clamp(torch.softmax(vanilla_logits, dim=1), min=1e-7)\n    a_prob = torch.clamp(torch.softmax(logits, dim=1), min=1e-7)\n\n    n_entropy = -torch.sum(n_prob * torch.log(n_prob), dim=1) / t\n    a_entropy = -torch.sum(a_prob * torch.log(a_prob), dim=1) / t\n\n    entropy_disimilarity = F.mse_loss(input=a_entropy, target=n_entropy, reduction=\"none\")\n    assert ~torch.isnan(entropy_disimilarity).any(), print(torch.min(n_entropy), torch.max(a_entropy))\n\n    return entropy_disimilarity\n\n\ndef energy_loss(logits, targets, vanilla_logits, out_idx=13, t=1.):\n    out_msk = (targets == out_idx)\n    void_msk = (targets == 255)\n\n    pseudo_targets = torch.argmax(vanilla_logits, dim=1)\n    outlier_msk = (out_msk | void_msk)\n    entropy_part = F.cross_entropy(input=logits, target=pseudo_targets, reduction='none')[~outlier_msk]\n    reg = disimilarity_entropy(logits=logits, vanilla_logits=vanilla_logits)[~outlier_msk]\n    if torch.sum(out_msk) > 0:\n        logits = logits.flatten(start_dim=2).permute(0, 2, 1)\n        energy_part = F.relu(torch.log(torch.sum(torch.exp(logits),dim=2))[out_msk.flatten(start_dim=1)]).mean()\n    else:\n        energy_part = torch.tensor([.0], device=targets.device)\n    return {\"entropy_part\": entropy_part.mean(), \"reg\": reg.mean(), \"energy_part\": energy_part}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from copy import deepcopy\n\nclass RPLDeepLab(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        \n        self.encoder = self.copy_un_freeze_params(model.encoder, unfreeze=False)\n        self.decoder = self.copy_un_freeze_params(model.decoder, unfreeze=False)\n        self.final = nn.Sequential(\n            self.copy_un_freeze_params(model.decoder.block2, unfreeze=False),\n            self.copy_un_freeze_params(model.segmentation_head, unfreeze=False),    \n        )\n        \n        self.atten_aspp_final = nn.Conv2d(256, 304, kernel_size=1, bias=False)\n        \n        self.residual_anomaly_block = nn.Sequential(\n            self.copy_un_freeze_params(model.decoder.aspp, unfreeze=True),\n            self.copy_un_freeze_params(model.decoder.up, unfreeze=True),\n            self.atten_aspp_final\n        )\n\n    def copy_un_freeze_params(self, layer: nn.Module, unfreeze: bool=True) -> nn.Module:\n        \"\"\"\n        function that create a deepcopy of a layer and unfreeze its parameters if unfreeze is True, otherwise freeze it\n\n        return: deepcopy of the layer freezed or unfreezed\n        \"\"\"\n        layer_copy = deepcopy(layer)\n        for param in layer_copy.parameters():\n            param.requires_grad = unfreeze\n        return layer_copy\n\n    def forward(self, x):\n\n        features = self.encoder(x)\n        aspp_features = self.decoder.aspp(features[-1])\n        aspp_features = self.decoder.up(aspp_features)\n        high_res_features = self.decoder.block1(features[2])\n        concat_features = torch.cat([aspp_features, high_res_features], dim=1)\n        \n        res = self.residual_anomaly_block(features[-1])\n\n        out1 = self.final(concat_features)\n        out2 = self.final(concat_features + res)\n\n        return out1, out2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def resume_run(run_id, model_name):\n\n    api = wandb.Api()\n    run = api.run(run_id)\n    \n    files = run.files()\n    for f in files:\n        if f.name.startswith(\"ckpts/\"):  # or adjust the filter\n            f.download(replace=True)\n    directory = f\"/kaggle/working/ckpts/{model_name}.pt\"\n    return directory","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = get_deeplab_model(encoder_name=\"efficientnet-b0\", encoder_weights = \"imagenet\", activation= None, num_classes= 13)\n\nrun_id = \"chri-project/ML4CV--assignment/cn4tyspf\"\nmodel_name = \"deeplabv3plus_efficientnet_b0-continue\"\ndirectory = resume_run(run_id, model_name)\nmodel_weigths = torch.load(directory, weights_only=True)\nmodel.load_state_dict(model_weigths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchinfo import summary\nrpl = RPLDeepLab(model)\nsummary(rpl, input_size= (8, 3, 512, 512))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"rpl_deeplabv3plus_efficientnet_b0\"\n\nrpl_cfg = {\n    \"num_epochs\" : 200,\n    \"lr\": 2e-4,\n    \"wd\": 0.001,\n    \"patience\": 1000,\n    \"loss\" : \"CrossEntropyLoss\",\n    \"optimizer\" : \"AdamW\",\n    \"data_augmentation\" : \"transforms.v2.RandomCrop(image_resize_scale),transforms.v2.RandomHorizontalFlip(), transforms.v2.Resize(image_resize_scale, transforms.InterpolationMode.NEAREST)\",\n    \"train\" : \"augmented with anomalies\",\n    \"validation\" : \"augmented with anomalies\",\n    \"run_name\": model_name,\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## model head need to remain at 13","metadata":{}},{"cell_type":"code","source":"run_id_rpl = \"chri-project/ML4CV--assignment/c8ij9i65\"\nrpl_model_name = \"rpl_deeplabv3plus_efficientnet_b0_first_try\"\ndirectory = resume_run(run_id_rpl, rpl_model_name)\nmodel_weights_path = torch.load(directory, weights_only=True)\nrpl.load_state_dict(model_weights_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rpl_trainer = Trainer(\n    model= rpl,\n    train_loader= train_dl,\n    val_loader= val_dl ,\n    loss = energy_loss,\n    device= DEVICE,\n    num_classes = len(COLORS),\n    model_name = model_name,\n    cfg= rpl_cfg,\n    #resume_ckpt = torch.load(resume_ckpt)\n)\n\n#rpl_trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\ndef plot_anomaly_heatmap(anomaly_score, image=None, title=\"Anomaly Heatmap\"):\n    \"\"\"\n    Plots a heatmap where red = high anomaly, blue = low anomaly.\n    \n    Parameters:\n    - anomaly_score: torch.Tensor or np.array of shape [H, W]\n    - image: optional RGB image [H, W, 3] in range [0, 1] or [0, 255]\n    - title: string title for plot\n    \"\"\"\n    if isinstance(anomaly_score, torch.Tensor):\n        anomaly_score = anomaly_score.squeeze().cpu().numpy()\n    \n    # Normalize to [0, 1]\n    anomaly_score = (anomaly_score - anomaly_score.min()) / (anomaly_score.max() - anomaly_score.min())\n\n\n    if image is not None:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n        #if isinstance(image, torch.Tensor):\n            #image = image.permute(1, 2, 0).cpu().numpy()\n        if image.max() > 1.0:\n            image = image / 255.0\n\n        \n        image1 = ax[0].imshow(anomaly_score, cmap='jet')  # 'jet': blue -> red\n        fig.colorbar(image1, ax=ax[0], label='Anomaly Score')\n        #ax[0].colorbar(label=\"Anomaly Score\")\n        ax[0].set_title(title)\n        ax[0].axis(\"off\")\n        \n        visualize_scene(image, ax[1])\n\n        \n    else:\n        \n        plt.imshow(anomaly_score, cmap='jet')\n\n        plt.colorbar(label=\"Anomaly Score\")\n        plt.title(title)\n        plt.axis(\"off\")\n        \n    plt.figure(figsize=(10, 12))\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef get_predictions(model, img):\n    img = img.unsqueeze(0).to(DEVICE)\n    pred = model(img)\n    inlier, outlier = pred\n    outlier = outlier.unsqueeze(0)\n    img = img.squeeze(0).cpu()\n    anomaly_score = compute_anomaly_score(outlier, mode=\"energy\").cpu()\n    plot_anomaly_heatmap(anomaly_score=anomaly_score, image=None) #try alpha= None\n    '''inlier, outlier = inlier.cpu(), outlier.cpu()\n    inlier = torch.argmax(inlier, dim=1)\n    inlier = inlier.squeeze(0)\n    outlier = torch.argmax(outlier, dim=1)\n    outlier = outlier.squeeze(0)\n    return inlier, outlier'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = val_dataset[1000]['image']\nget_predictions(rpl, img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = test_dataset[1024]['image']\nget_predictions(rpl, img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"res = predict(rpl, val_dl)\nprint(res)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"res = predict(rpl, test_dl)\nprint(res)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"rpl: 0.5121710741219887 <br>\nmaximum softmax probability: <br>\nmax logit: <br>\nstandardized max logit: ","metadata":{}},{"cell_type":"code","source":"dataset = val_dataset\nfig, ax = plt.subplots(1, 3)\n\nidx = 500\nimg, lbl = dataset[idx].values()\ninlier, outlier = get_predictions(img, rpl)\nvisualize_annotation(inlier, ax[0])\nvisualize_annotation(outlier, ax[1])\nvisualize_annotation(lbl, ax[2])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}