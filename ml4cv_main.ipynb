{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11972215,"sourceType":"datasetVersion","datasetId":6205623}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":43168.20753,"end_time":"2025-05-20T05:43:13.379570","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-19T17:43:45.172040","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0953ba74a7804f0c8ab0a86292e20f3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0bc9c92571c64725bd65b6bd94442c09":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1132a3c3c9424ef18637a5247c0ce240":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_8740d4ebd63f45b7b582a9dff67b6b93","max":21355856,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e6225ade8284d30ae074f2242879e63","tabbable":null,"tooltip":null,"value":21355856}},"162b28b12a03485c882acfd841432326":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"214c981d0ed0477783550a8b584a3a9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"561478ec9da948618547410669721420":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"644cf79717d943d9b3bbebbd9fc77a5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_da69f006243745109f8377c8d3cc675e","max":106,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0953ba74a7804f0c8ab0a86292e20f3a","tabbable":null,"tooltip":null,"value":106}},"724a862ebbf94aa7adc87c3c91491bba":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"790a8c36e1934450a2f824477b4849eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8bb70527cabe478d90410ab3dbe43de7","IPY_MODEL_644cf79717d943d9b3bbebbd9fc77a5d","IPY_MODEL_ea92b56cb4254a28aad377569fcd6b42"],"layout":"IPY_MODEL_add1776c4a8242dca7b1e05059b60497","tabbable":null,"tooltip":null}},"8740d4ebd63f45b7b582a9dff67b6b93":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bb70527cabe478d90410ab3dbe43de7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_0bc9c92571c64725bd65b6bd94442c09","placeholder":"​","style":"IPY_MODEL_c2bdc6a2754b45ef9e1af4dc9d1527be","tabbable":null,"tooltip":null,"value":"config.json: 100%"}},"8bcaa3c0930f43b28338cb581e2f9318":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e6225ade8284d30ae074f2242879e63":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9a3055835f0f4329b6446f29ed37bf10":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_8bcaa3c0930f43b28338cb581e2f9318","placeholder":"​","style":"IPY_MODEL_162b28b12a03485c882acfd841432326","tabbable":null,"tooltip":null,"value":"model.safetensors: 100%"}},"a36448319e334c719b54154d081beb44":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad09fd47a26f43488fd3cfbfc38db7b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_deea08256d214659a2efdb250619e3ca","placeholder":"​","style":"IPY_MODEL_561478ec9da948618547410669721420","tabbable":null,"tooltip":null,"value":" 21.4M/21.4M [00:00&lt;00:00, 150MB/s]"}},"add1776c4a8242dca7b1e05059b60497":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2bdc6a2754b45ef9e1af4dc9d1527be":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"da69f006243745109f8377c8d3cc675e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"deea08256d214659a2efdb250619e3ca":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea92b56cb4254a28aad377569fcd6b42":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_a36448319e334c719b54154d081beb44","placeholder":"​","style":"IPY_MODEL_214c981d0ed0477783550a8b584a3a9e","tabbable":null,"tooltip":null,"value":" 106/106 [00:00&lt;00:00, 11.8kB/s]"}},"f0fba40be3be429d82cf62d09e9362ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9a3055835f0f4329b6446f29ed37bf10","IPY_MODEL_1132a3c3c9424ef18637a5247c0ce240","IPY_MODEL_ad09fd47a26f43488fd3cfbfc38db7b7"],"layout":"IPY_MODEL_724a862ebbf94aa7adc87c3c91491bba","tabbable":null,"tooltip":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"### Author\n0001128790 - Christian Di Buò - christian.dibuo@studio.unibo.it","metadata":{"papermill":{"duration":0.017128,"end_time":"2025-05-19T17:43:49.432714","exception":false,"start_time":"2025-05-19T17:43:49.415586","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### installing required dependencies","metadata":{}},{"cell_type":"code","source":"!pip install segmentation-models-pytorch albumentations --no-deps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:31:43.071233Z","iopub.execute_input":"2025-06-06T20:31:43.071473Z","iopub.status.idle":"2025-06-06T20:31:45.707260Z","shell.execute_reply.started":"2025-06-06T20:31:43.071445Z","shell.execute_reply":"2025-06-06T20:31:45.706363Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install fvcore","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:31:45.709035Z","iopub.execute_input":"2025-06-06T20:31:45.709255Z","iopub.status.idle":"2025-06-06T20:31:53.132854Z","shell.execute_reply.started":"2025-06-06T20:31:45.709233Z","shell.execute_reply":"2025-06-06T20:31:53.132173Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Importing images","metadata":{"papermill":{"duration":0.014766,"end_time":"2025-05-19T17:43:49.463263","exception":false,"start_time":"2025-05-19T17:43:49.448497","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os, json\nimport subprocess, sys\nimport random\nimport math\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport copy\nfrom copy import deepcopy\nimport cv2\nfrom PIL import Image, ImageDraw\n\nfrom sklearn.metrics import average_precision_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nfrom torchvision import transforms\nfrom torchvision.transforms import v2\nfrom transformers import MobileViTImageProcessor, MobileViTForSemanticSegmentation\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport segmentation_models_pytorch as smp\nfrom segmentation_models_pytorch import losses\nfrom torchinfo import summary\nfrom abc import ABC\nimport pathlib, torchvision","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-06-06T20:31:53.133833Z","iopub.execute_input":"2025-06-06T20:31:53.134047Z","iopub.status.idle":"2025-06-06T20:32:20.153297Z","shell.execute_reply.started":"2025-06-06T20:31:53.134025Z","shell.execute_reply":"2025-06-06T20:32:20.152503Z"},"papermill":{"duration":29.356241,"end_time":"2025-05-19T17:44:18.834419","exception":false,"start_time":"2025-05-19T17:43:49.478178","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\n\nWANDB_USER = \"chri-project\"\nWANDB_PROJECT = \"ML4CV--assignment\"\nwandb.login(key='2b387b514b9fcec8902df2b863ae0646f56125d6')","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:32:20.154184Z","iopub.execute_input":"2025-06-06T20:32:20.154738Z","iopub.status.idle":"2025-06-06T20:32:27.773974Z","shell.execute_reply.started":"2025-06-06T20:32:20.154710Z","shell.execute_reply":"2025-06-06T20:32:27.773259Z"},"papermill":{"duration":2.754731,"end_time":"2025-05-19T17:44:26.161644","exception":false,"start_time":"2025-05-19T17:44:23.406913","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fix_random(seed: int) -> None:\n    \"\"\"\n    Fix all the possible sources of randomness.\n\n    Args:\n        seed: the seed to use.\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nfix_random(seed=42)","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:32:27.774790Z","iopub.execute_input":"2025-06-06T20:32:27.775427Z","iopub.status.idle":"2025-06-06T20:32:27.784356Z","shell.execute_reply.started":"2025-06-06T20:32:27.775407Z","shell.execute_reply":"2025-06-06T20:32:27.783631Z"},"papermill":{"duration":0.028011,"end_time":"2025-05-19T17:44:28.120593","exception":false,"start_time":"2025-05-19T17:44:28.092582","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:32:27.785096Z","iopub.execute_input":"2025-06-06T20:32:27.785286Z","iopub.status.idle":"2025-06-06T20:32:27.793597Z","shell.execute_reply.started":"2025-06-06T20:32:27.785266Z","shell.execute_reply":"2025-06-06T20:32:27.792838Z"},"papermill":{"duration":0.021527,"end_time":"2025-05-19T17:44:28.158515","exception":false,"start_time":"2025-05-19T17:44:28.136988","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nSource: https://github.com/hendrycks/anomaly-seg/issues/15#issuecomment-890300278\n\"\"\"\nCOLORS = np.array([\n    [  0,   0,   0],  # unlabeled    =   0,\n    [ 70,  70,  70],  # building     =   1,\n    [190, 153, 153],  # fence        =   2, \n    [250, 170, 160],  # other        =   3,\n    [220,  20,  60],  # pedestrian   =   4, \n    [153, 153, 153],  # pole         =   5,\n    [157, 234,  50],  # road line    =   6, \n    [128,  64, 128],  # road         =   7,\n    [244,  35, 232],  # sidewalk     =   8,\n    [107, 142,  35],  # vegetation   =   9, \n    [  0,   0, 142],  # car          =  10,\n    [102, 102, 156],  # wall         =  11, \n    [220, 220,   0],  # traffic sign =  12,\n    [ 60, 250, 240],  # anomaly      =  13,\n]) ","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:32:27.795866Z","iopub.execute_input":"2025-06-06T20:32:27.796516Z","iopub.status.idle":"2025-06-06T20:32:27.808181Z","shell.execute_reply.started":"2025-06-06T20:32:27.796493Z","shell.execute_reply":"2025-06-06T20:32:27.807514Z"},"papermill":{"duration":1.068032,"end_time":"2025-05-19T17:44:29.242745","exception":false,"start_time":"2025-05-19T17:44:28.174713","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TODO: Show the imbalance of the classes, if any. In this way you can justify that there are errors","metadata":{"execution":{"iopub.execute_input":"2025-04-23T14:06:25.487891Z","iopub.status.busy":"2025-04-23T14:06:25.487534Z","iopub.status.idle":"2025-04-23T14:06:25.491225Z","shell.execute_reply":"2025-04-23T14:06:25.490484Z","shell.execute_reply.started":"2025-04-23T14:06:25.487871Z"},"papermill":{"duration":0.015911,"end_time":"2025-05-19T17:44:29.275608","exception":false,"start_time":"2025-05-19T17:44:29.259697","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class StreetHazardsDataset(Dataset):\n    def __init__(self, odgt_file, image_resize=(512, 896), spatial_transforms=None, images_only_transforms=None):\n        \"\"\"\n        Args:\n            odgt_file (str): Path to the .odgt file (train, val, or test).\n            transform (callable, optional): Transformations to apply to images and masks.\n            compute_dist_map: is used to pre-compute the distance maps to use then in the Boundary loss.\n        \"\"\"\n\n        self.spatial_transforms = spatial_transforms\n        self.images_only_transforms = images_only_transforms\n        self.image_resize = image_resize\n\n        # Load the .odgt file\n        with open(odgt_file, \"r\") as f:\n            odgt_data = json.load(f)\n        \n\n        self.paths = [\n            {\n                \"image\": os.path.join(Path(odgt_file).parent, data[\"fpath_img\"]),\n                \"labels\": os.path.join(Path(odgt_file).parent, data[\"fpath_segm\"]),\n            }\n            for data in odgt_data \n        ]\n    \n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, idx):\n\n        image = Image.open(self.paths[idx][\"image\"]).convert(\"RGB\")\n        labels = Image.open(self.paths[idx][\"labels\"])\n\n        #to_tensor\n        #image = transforms.ToTensor()(image)\n        #labels = torch.as_tensor(transforms.functional.pil_to_tensor(labels), dtype=torch.int64) - 1\n\n        if self.image_resize:\n            image = transforms.Resize(self.image_resize, transforms.InterpolationMode.BILINEAR)(image)\n            labels = transforms.Resize(self.image_resize, transforms.InterpolationMode.NEAREST)(labels)\n            \n        if self.spatial_transforms:\n            image, labels  = self.spatial_transforms(image, labels)         \n\n        #to_tensor\n        image = transforms.ToTensor()(image)\n        labels = torch.as_tensor(transforms.functional.pil_to_tensor(labels), dtype=torch.int64) - 1\n        \n        labels = labels.squeeze(0)\n        \n        if self.images_only_transforms:\n            image = self.images_only_transforms(image)\n\n        return {'image' : image, 'labels' : labels}","metadata":{"papermill":{"duration":0.028385,"end_time":"2025-05-19T17:44:29.351694","exception":false,"start_time":"2025-05-19T17:44:29.323309","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:32:27.808817Z","iopub.execute_input":"2025-06-06T20:32:27.809035Z","iopub.status.idle":"2025-06-06T20:32:27.820372Z","shell.execute_reply.started":"2025-06-06T20:32:27.809019Z","shell.execute_reply":"2025-06-06T20:32:27.819713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_annotation(annotation_img: np.ndarray|torch.Tensor, ax=None):\n    \"\"\"\n    Adapted from https://github.com/CVLAB-Unibo/ml4cv-assignment/blob/master/utils/visualize.py\n    \"\"\"\n    if ax is None: ax = plt.gca()\n    annotation_img = np.asarray(annotation_img)\n    img_new = np.zeros((*annotation_img.shape, 3))\n\n    for index, color in enumerate(COLORS):\n        img_new[annotation_img == index] = color\n\n    ax.imshow(img_new / 255.0)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\ndef visualize_scene(image: np.ndarray|torch.Tensor, ax=None):\n    if ax is None: ax = plt.gca()\n    image = np.asarray(image)\n    ax.imshow(np.moveaxis(image, 0, -1))\n    ax.set_xticks([])\n    ax.set_yticks([])","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:32:27.821329Z","iopub.execute_input":"2025-06-06T20:32:27.821575Z","iopub.status.idle":"2025-06-06T20:32:27.834647Z","shell.execute_reply.started":"2025-06-06T20:32:27.821554Z","shell.execute_reply":"2025-06-06T20:32:27.833954Z"},"papermill":{"duration":0.022219,"end_time":"2025-05-19T17:44:29.389760","exception":false,"start_time":"2025-05-19T17:44:29.367541","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MixDataset(Dataset):\n    def __init__(self, inlier_dataset, outlier_dataset, images_only_transforms= None, anomaly_probability=1, max_anomalies = 4, anomaly_idx = 13):\n        \n        self.inlier_dataset = inlier_dataset\n        self.outlier_dataset = outlier_dataset\n        self.anomaly_idx = anomaly_idx\n        self.anomaly_probability = anomaly_probability\n        self.max_anomalies = max_anomalies\n        self.images_only_transforms = images_only_transforms\n\n    def inject_anomalies(self, sh_img, sh_lbl):\n\n        n_anomalies = random.randint(1, self.max_anomalies)\n        for i in range(n_anomalies):\n\n            rand_idx = random.randint(0, len(self.outlier_dataset)-1)\n\n            anomaly_size = (np.random.randint(sh_img.shape[1]*0.1, sh_img.shape[1]*0.3), np.random.randint(sh_img.shape[2]*0.1, sh_img.shape[2]*0.3))\n            i, j, h, w = transforms.RandomCrop.get_params(sh_img, output_size=anomaly_size)\n            possible_classes = []\n            while len(possible_classes) == 0: # In some cases there are no classes available\n                anomaly_idx = np.random.randint(0, len(self.outlier_dataset))\n                anomaly_image = transforms.ToTensor()(self.outlier_dataset[anomaly_idx][0])\n                anomaly_annot = torch.from_numpy(np.array(self.outlier_dataset[anomaly_idx][1])).unsqueeze(0)\n                possible_classes = np.unique(anomaly_annot)[1:-1]\n\n            anomaly_class = np.random.choice(possible_classes)\n            anomaly_image = F.interpolate(anomaly_image.unsqueeze(0), size=(h, w), mode=\"bilinear\").squeeze(0)\n            anomaly_annot = F.interpolate(anomaly_annot.unsqueeze(0), size=(h, w), mode=\"nearest\").squeeze((0, 1))\n\n            # Insert anomaly\n            sh_img[:, i:i+h, j:j+w][:, anomaly_annot == anomaly_class] = anomaly_image[:, anomaly_annot == anomaly_class]\n            sh_lbl[i:i+h, j:j+w][anomaly_annot == anomaly_class] = self.anomaly_idx\n            anomaly_annot[anomaly_annot == anomaly_class] = self.anomaly_idx\n            anomaly_annot[anomaly_annot != self.anomaly_idx] = 0\n\n        return sh_img, sh_lbl, anomaly_image, anomaly_annot\n\n    def __len__(self):\n        return len(self.inlier_dataset)\n\n    def __getitem__(self, idx):\n\n        sh_img, sh_lbl = self.inlier_dataset[idx].values()\n        p = random.random()\n        if p < self.anomaly_probability:\n            sh_img, sh_lbl, ood_img, ood_lbl = self.inject_anomalies(sh_img, sh_lbl)\n            w, h = sh_lbl.shape[0], sh_lbl.shape[1]\n            ood_img = transforms.Resize((w, h), transforms.InterpolationMode.BILINEAR)(ood_img)\n            ood_lbl = transforms.Resize((w, h), transforms.InterpolationMode.NEAREST)(ood_lbl.unsqueeze(0)).squeeze(0)\n\n        if self.images_only_transforms:\n            sh_img = self.images_only_transforms(sh_img)\n            \n        return {\"image\" : sh_img,\"labels\": sh_lbl}\n        #return sh_img, sh_lbl, ood_img, ood_lbl","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:32:27.835376Z","iopub.execute_input":"2025-06-06T20:32:27.835575Z","iopub.status.idle":"2025-06-06T20:32:27.853382Z","shell.execute_reply.started":"2025-06-06T20:32:27.835562Z","shell.execute_reply":"2025-06-06T20:32:27.852769Z"},"papermill":{"duration":0.026073,"end_time":"2025-05-19T17:44:29.469395","exception":false,"start_time":"2025-05-19T17:44:29.443322","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_mean_std(loader):\n    \"\"\"\n    compute mean and standard deviation to normalize the images; can be used in alternative of imagenet mean and std.\n    (seems to worse results)\n    \"\"\"\n    mean = 0.0\n    std = 0.0\n    nb_samples = 0\n\n    for batch in tqdm(loader):\n        images = batch[\"image\"]\n        batch_samples = images.size(0)\n        images = images.view(batch_samples, images.size(1), -1) \n    \n        mean += images.mean(2).sum(0)  \n        std += images.std(2).sum(0)\n        nb_samples += batch_samples \n        del batch\n\n    mean /= nb_samples\n    std /= nb_samples\n    return mean, std","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:32:27.854065Z","iopub.execute_input":"2025-06-06T20:32:27.854266Z","iopub.status.idle":"2025-06-06T20:32:27.870806Z","shell.execute_reply.started":"2025-06-06T20:32:27.854251Z","shell.execute_reply":"2025-06-06T20:32:27.869978Z"},"papermill":{"duration":0.022634,"end_time":"2025-05-19T17:44:29.593803","exception":false,"start_time":"2025-05-19T17:44:29.571169","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_class_frequency(pixels_per_class, total_pixels, ax):\n    percentages = (pixels_per_class / total_pixels) * 100\n    class_names = list(range(len(pixels_per_class)))\n    \n    bars = ax.barh(class_names, percentages, color='skyblue')\n\n    ax.set_ylabel('Class Name', fontsize=12)\n    ax.set_xlabel('Total Pixel Count', fontsize=12)\n    ax.set_title('Pixel Frequencies Per Class', fontsize=14)\n    ax.set_yticks(class_names)\n    ax.set_yticklabels(class_names, rotation=0, fontsize=10)\n    ax.tick_params(axis='x', labelsize=10)\n\n    for bar in bars:\n        xval = bar.get_width()\n        ax.text(xval + (max(percentages) * 0.01), \n                 bar.get_y() + bar.get_height() / 2,\n                 f'{xval:.2f}%', ha='left', va='center', fontsize=9)\n\n    # Optional: return bars if needed for further customization\n    return bars\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:32:27.871536Z","iopub.execute_input":"2025-06-06T20:32:27.872058Z","iopub.status.idle":"2025-06-06T20:32:27.884208Z","shell.execute_reply.started":"2025-06-06T20:32:27.872036Z","shell.execute_reply":"2025-06-06T20:32:27.883615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_class_frequency(dataset, num_classes, normalize = False, plot_frequencies = False, ax= None):\n    \"\"\"\n    count the number of pixels belonging to each class\n    return: the computed weights\n    \"\"\"\n    data_loader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=2)\n    pixels_per_class = torch.zeros(num_classes, dtype=torch.long)\n\n    for batch in tqdm(data_loader):\n        labels = batch['labels']  # Assumes shape: (B, H, W)\n        labels = labels.view(-1)  # Flatten all pixels\n        counts = torch.bincount(labels, minlength=num_classes)\n        pixels_per_class += counts\n\n    total_pixels = pixels_per_class.sum()\n    frequencies = pixels_per_class.float() / total_pixels\n    weights = 1.0 / frequencies\n    if normalize:\n        weights = weights / weights.sum() * len(weights)\n\n    if plot_frequencies:\n        plot_class_frequency(pixels_per_class, total_pixels, ax)\n\n    return weights\n\n#weight_ce = compute_class_frequency(dataset= train_dataset, num_classes=13, normalize = True)\n#print(weight_ce)","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:32:27.884956Z","iopub.execute_input":"2025-06-06T20:32:27.885176Z","iopub.status.idle":"2025-06-06T20:32:27.899203Z","shell.execute_reply.started":"2025-06-06T20:32:27.885158Z","shell.execute_reply":"2025-06-06T20:32:27.898310Z"},"papermill":{"duration":0.02289,"end_time":"2025-05-19T17:44:29.819801","exception":false,"start_time":"2025-05-19T17:44:29.796911","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"anomaly_dataset_path = './data_voc'\nvoc_train = torchvision.datasets.VOCSegmentation(anomaly_dataset_path, image_set=\"train\", download=True)\nvoc_val = torchvision.datasets.VOCSegmentation(anomaly_dataset_path, image_set=\"val\", download=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:32:27.899858Z","iopub.execute_input":"2025-06-06T20:32:27.900070Z","iopub.status.idle":"2025-06-06T20:35:46.934440Z","shell.execute_reply.started":"2025-06-06T20:32:27.900055Z","shell.execute_reply":"2025-06-06T20:35:46.933503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def as_numpy(obj):\n    if torch.is_tensor(obj):\n        return obj.cpu().numpy()\n    else:\n        return np.array(obj)","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:35:46.935467Z","iopub.execute_input":"2025-06-06T20:35:46.936052Z","iopub.status.idle":"2025-06-06T20:35:46.940127Z","shell.execute_reply.started":"2025-06-06T20:35:46.936022Z","shell.execute_reply":"2025-06-06T20:35:46.939358Z"},"papermill":{"duration":0.027733,"end_time":"2025-05-19T17:44:31.481456","exception":false,"start_time":"2025-05-19T17:44:31.453723","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AUPR:\n    def __init__(self, anomaly_idx = 13):\n\n        self.mean_aupr = []\n        self.anomaly_idx = anomaly_idx\n\n    def update(self, anomaly_score, labels):\n    \n        for i in range(anomaly_score.shape[0]):\n            \n            preds, lbl = anomaly_score[i], (labels[i] == self.anomaly_idx)\n            \n            if preds.dim() == 2: preds = preds.unsqueeze(0)\n            if lbl.dim() == 2: lbl = lbl.unsqueeze(0)\n            preds, lbl = preds.cpu(), lbl.cpu()\n            self.mean_aupr.append(average_precision_score(lbl.type(torch.int32).flatten().numpy(), preds.type(torch.float32).flatten().numpy()))\n            \n    def get_results(self):\n        \n        return sum(self.mean_aupr)/len(self.mean_aupr) * 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:46.940970Z","iopub.execute_input":"2025-06-06T20:35:46.941660Z","iopub.status.idle":"2025-06-06T20:35:47.362657Z","shell.execute_reply.started":"2025-06-06T20:35:46.941637Z","shell.execute_reply":"2025-06-06T20:35:47.361950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MeanIoU:\n    \"\"\"\n    taken from https://github.com/Jun-CEN/Open-World-Semantic-Segmentation/blob/main/DeepLabV3Plus-Pytorch/metrics/stream_metrics.py\n    \"\"\"\n    def __init__(self, n_classes= 13):\n        self.n_classes = n_classes\n        self.confusion_matrix = np.zeros((n_classes, n_classes))\n        \n    def update(self, label_trues, logits):\n        label_preds = torch.argmax(logits, dim=1)\n        label_preds, label_trues = label_preds.cpu().numpy(), label_trues.cpu().numpy()\n        for lt, lp in zip(label_trues, label_preds):\n            self.confusion_matrix += self._fast_hist( lt.flatten(), lp.flatten())\n\n    def _fast_hist(self, label_true, label_pred):\n        mask = (label_true >= 0) & (label_true < self.n_classes)\n        hist = np.bincount(\n            self.n_classes * label_true[mask].astype(int) + label_pred[mask],\n            minlength=self.n_classes ** 2,\n        ).reshape(self.n_classes, self.n_classes)\n        return hist\n\n    def get_results(self):\n        \"\"\"Returns accuracy score evaluation result.\n            - overall accuracy\n            - mean accuracy\n            - mean IU\n            - fwavacc\n        \"\"\"\n        hist = self.confusion_matrix\n        acc = np.diag(hist).sum() / hist.sum()\n        acc_cls = np.diag(hist) / hist.sum(axis=1)\n        acc_cls = np.nanmean(acc_cls)\n        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n        mean_iu = np.nanmean(iu, axis= 0)\n        freq = hist.sum(axis=1) / hist.sum()\n        fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n        cls_iu = dict(zip(range(self.n_classes), iu))\n\n        return {\n                #\"Overall Acc\": acc,\n                #\"Mean Acc\": acc_cls,\n                #\"FreqW Acc\": fwavacc,\n                \"Mean IoU\": mean_iu,\n                \"Class IoU\": cls_iu,\n            }\n","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:35:47.363374Z","iopub.execute_input":"2025-06-06T20:35:47.363672Z","iopub.status.idle":"2025-06-06T20:35:47.773465Z","shell.execute_reply.started":"2025-06-06T20:35:47.363654Z","shell.execute_reply":"2025-06-06T20:35:47.772703Z"},"papermill":{"duration":0.032064,"end_time":"2025-05-19T17:44:31.600102","exception":false,"start_time":"2025-05-19T17:44:31.568038","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kornia.morphology import dilation, erosion\nfrom scipy import ndimage as ndi\n\nd_k1 = torch.zeros((1, 1, 2 * 1 + 1, 2 * 1 + 1)).cuda()\nd_k2 = torch.zeros((1, 1, 2 * 2 + 1, 2 * 2 + 1)).cuda()\nd_k3 = torch.zeros((1, 1, 2 * 3 + 1, 2 * 3 + 1)).cuda()\nd_k4 = torch.zeros((1, 1, 2 * 4 + 1, 2 * 4 + 1)).cuda()\nd_k5 = torch.zeros((1, 1, 2 * 5 + 1, 2 * 5 + 1)).cuda()\nd_k6 = torch.zeros((1, 1, 2 * 6 + 1, 2 * 6 + 1)).cuda()\nd_k7 = torch.zeros((1, 1, 2 * 7 + 1, 2 * 7 + 1)).cuda()\nd_k8 = torch.zeros((1, 1, 2 * 8 + 1, 2 * 8 + 1)).cuda()\nd_k9 = torch.zeros((1, 1, 2 * 9 + 1, 2 * 9 + 1)).cuda()\n\nd_ks = {1: d_k1, 2: d_k2, 3: d_k3, 4: d_k4, 5: d_k5, 6: d_k6, 7: d_k7, 8: d_k8, 9: d_k9}\n\n\nselem = torch.ones((3, 3)).cuda()\nselem_dilation = torch.FloatTensor(ndi.generate_binary_structure(2, 1)).cuda()\n\nfor k, v in d_ks.items():\n    v[:,:,k,k] = 1\n    for i in range(k):\n        v = dilation(v, selem_dilation)\n    d_ks[k] = v.squeeze(0).squeeze(0)\n\ndef find_boundaries(labels):\n    \"\"\"\n    Calculate boundary mask by getting diff of dilated and eroded prediction maps\n    \"\"\"\n    assert len(labels.shape) == 4\n    boundaries = (dilation(labels.float(), selem_dilation) != erosion(labels.float(), selem)).float()\n    ### save_image(boundaries, f'boundaries_{boundaries.float().mean():.2f}.png', normalize=True)\n\n    return boundaries\n\ndef expand_boundaries(boundaries, r=0):\n    \"\"\"\n    Expand boundary maps with the rate of r\n    \"\"\"\n    if r == 0:\n        return boundaries\n    expanded_boundaries = dilation(boundaries, d_ks[r])\n    ### save_image(expanded_boundaries, f'expanded_boundaries_{r}_{boundaries.float().mean():.2f}.png', normalize=True)\n    return expanded_boundaries","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:35:47.774350Z","iopub.execute_input":"2025-06-06T20:35:47.774653Z","iopub.status.idle":"2025-06-06T20:35:49.484686Z","shell.execute_reply.started":"2025-06-06T20:35:47.774632Z","shell.execute_reply":"2025-06-06T20:35:49.483877Z"},"papermill":{"duration":1.386385,"end_time":"2025-05-19T17:44:33.076452","exception":false,"start_time":"2025-05-19T17:44:31.690067","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BoundarySuppressionWithSmoothing(nn.Module):\n    \"\"\"\n    Apply boundary suppression and dilated smoothing\n    \"\"\"\n    def __init__(self, boundary_suppression=True, boundary_width=4, boundary_iteration=4,\n                 dilated_smoothing=True, kernel_size=7, dilation=6):\n        super(BoundarySuppressionWithSmoothing, self).__init__()\n        self.kernel_size = kernel_size\n        self.dilation = dilation\n        self.boundary_suppression = boundary_suppression\n        self.boundary_width = boundary_width\n        self.boundary_iteration = boundary_iteration\n\n        sigma = 1.0\n        size = 7\n        gaussian_kernel = np.fromfunction(lambda x, y: (1/(2*math.pi*sigma**2)) * math.e ** ((-1*((x-(size-1)/2)**2+(y-(size-1)/2)**2))/(2*sigma**2)), (size, size))\n        gaussian_kernel /= np.sum(gaussian_kernel)\n        gaussian_kernel = torch.Tensor(gaussian_kernel).unsqueeze(0).unsqueeze(0)\n        self.dilated_smoothing = dilated_smoothing\n\n        self.first_conv = nn.Conv2d(1, 1, kernel_size=3, stride=1, bias=False)\n        self.first_conv.weight = torch.nn.Parameter(torch.ones_like((self.first_conv.weight)))\n\n        self.second_conv = nn.Conv2d(1, 1, kernel_size=self.kernel_size, stride=1, dilation=self.dilation, bias=False)\n        self.second_conv.weight = torch.nn.Parameter(gaussian_kernel)\n\n\n    def forward(self, x, prediction=None):\n        if len(x.shape) == 3:\n            x = x.unsqueeze(1)\n        x_size = x.size()\n        # B x 1 x H x W\n        assert len(x.shape) == 4\n        out = x\n        if self.boundary_suppression:\n            # obtain the boundary map of width 2 by default\n            # this can be calculated by the difference of dilation and erosion\n            boundaries = find_boundaries(prediction.unsqueeze(1))\n            expanded_boundaries = None\n            if self.boundary_iteration != 0:\n                assert self.boundary_width % self.boundary_iteration == 0\n                diff = self.boundary_width // self.boundary_iteration\n            for iteration in range(self.boundary_iteration):\n                if len(out.shape) != 4:\n                    out = out.unsqueeze(1)\n                prev_out = out\n                # if it is the last iteration or boundary width is zero\n                if self.boundary_width == 0 or iteration == self.boundary_iteration - 1:\n                    expansion_width = 0\n                # reduce the expansion width for each iteration\n                else:\n                    expansion_width = self.boundary_width - diff * iteration - 1\n                # expand the boundary obtained from the prediction (width of 2) by expansion rate\n                expanded_boundaries = expand_boundaries(boundaries, r=expansion_width)\n                # invert it so that we can obtain non-boundary mask\n                non_boundary_mask = 1. * (expanded_boundaries == 0)\n\n                f_size = 1\n                num_pad = f_size\n\n                # making boundary regions to 0\n                x_masked = out * non_boundary_mask\n                x_padded = nn.ReplicationPad2d(num_pad)(x_masked)\n\n                non_boundary_mask_padded = nn.ReplicationPad2d(num_pad)(non_boundary_mask)\n\n                # sum up the values in the receptive field\n                y = self.first_conv(x_padded)\n                # count non-boundary elements in the receptive field\n                num_calced_elements = self.first_conv(non_boundary_mask_padded)\n                num_calced_elements = num_calced_elements.long()\n\n                # take an average by dividing y by count\n                # if there is no non-boundary element in the receptive field,\n                # keep the original value\n                avg_y = torch.where((num_calced_elements == 0), prev_out, y / num_calced_elements)\n                out = avg_y\n\n                # update boundaries only\n                out = torch.where((non_boundary_mask == 0), out, prev_out)\n                del expanded_boundaries, non_boundary_mask\n\n            # second stage; apply dilated smoothing\n            if self.dilated_smoothing == True:\n                out = nn.ReplicationPad2d(self.dilation * 3)(out)\n                out = self.second_conv(out)\n\n            return out.squeeze(1)\n        else:\n            if self.dilated_smoothing == True:\n                out = nn.ReplicationPad2d(self.dilation * 3)(out)\n                out = self.second_conv(out)\n            else:\n                out = x\n\n        return out.squeeze(1)\n","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:35:49.485681Z","iopub.execute_input":"2025-06-06T20:35:49.485963Z","iopub.status.idle":"2025-06-06T20:35:49.499425Z","shell.execute_reply.started":"2025-06-06T20:35:49.485939Z","shell.execute_reply":"2025-06-06T20:35:49.498751Z"},"papermill":{"duration":0.036202,"end_time":"2025-05-19T17:44:33.137763","exception":false,"start_time":"2025-05-19T17:44:33.101561","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MaximumSoftmaxProbability(nn.Module):\n    def __init__(self, segmenter, multi_scale = None):\n        super().__init__()\n        self.segmenter = segmenter.eval().to(DEVICE)\n        self.multi_scale = multi_scale\n        if self.multi_scale:\n            self.multi_scale.to(DEVICE)\n\n    @torch.no_grad()\n    def forward(self, inputs):\n\n        logits = self.segmenter(inputs)\n        anomaly_score, prediction = torch.max(nn.functional.softmax(logits, dim=1),dim=1)\n        \n        anomaly_score = 1 - anomaly_score\n        \n        if self.multi_scale:\n            with torch.no_grad():\n                anomaly_score = self.multi_scale(anomaly_score, prediction)\n\n        return logits, anomaly_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:49.500124Z","iopub.execute_input":"2025-06-06T20:35:49.500400Z","iopub.status.idle":"2025-06-06T20:35:50.512930Z","shell.execute_reply.started":"2025-06-06T20:35:49.500376Z","shell.execute_reply":"2025-06-06T20:35:50.512135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MaxLogit(nn.Module):\n    def __init__(self, segmenter, multi_scale = None):\n        super().__init__()\n        self.segmenter = segmenter.eval().to(DEVICE)\n        self.multi_scale = multi_scale\n        if self.multi_scale:\n            self.multi_scale.to(DEVICE)\n\n    @torch.no_grad()\n    def forward(self, inputs):\n\n        logits = self.segmenter(inputs)\n        anomaly_score, prediction = torch.max(logits,dim=1)\n        \n        anomaly_score = 1 - anomaly_score\n        \n        if self.multi_scale:\n            with torch.no_grad():\n                anomaly_score = self.multi_scale(anomaly_score, prediction)\n\n        return logits, anomaly_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:50.513909Z","iopub.execute_input":"2025-06-06T20:35:50.514182Z","iopub.status.idle":"2025-06-06T20:35:51.226546Z","shell.execute_reply.started":"2025-06-06T20:35:50.514164Z","shell.execute_reply":"2025-06-06T20:35:51.225853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class StandardizedMaxLogit(nn.Module):\n    def __init__(self, segmenter, train_dl, multi_scale = None):\n        super().__init__()\n        self.segmenter = segmenter.eval().to(DEVICE)\n        \n        self.multi_scale = multi_scale\n        if self.multi_scale:\n            self.multi_scale.to(DEVICE)\n\n        self.class_mean, self.class_var = self.compute_mean_std(train_dl)\n\n    @torch.no_grad()\n    def compute_mean_std(self, train_dl, num_classes = 13):\n        \n        class_mean = np.zeros(num_classes)\n        class_std = np.zeros(num_classes)\n        iter_count = 0\n        \n        for batch in tqdm(train_dl, desc=\"Computing mean and std on train\"):\n            imgs = batch['image'].to(DEVICE)\n            #labels = batch['labels'].to(DEVICE)\n    \n            logits = self.segmenter(imgs)\n            \n            for logit in logits:\n                iter_count += 1\n                conf, labels = torch.max(logit, 0)\n                for c in range(num_classes):\n                    tens = torch.where(labels == c, conf, 0)\n                    mean, std = torch.std_mean(tens)\n        \n                    class_mean[c] += as_numpy(mean)\n                    class_std[c] += as_numpy(std)\n            \n        class_mean = class_mean/iter_count\n        class_std = class_std/iter_count\n        \n        return class_mean, class_std**2\n    \n    @torch.no_grad()\n    def forward(self, inputs):\n\n        logits = self.segmenter(inputs)\n        \n        anomaly_score, prediction  = torch.max(logits,dim=1)\n        for c in range(len(self.class_mean)):\n            anomaly_score = torch.where(\n                prediction == c,\n                (anomaly_score - self.class_mean[c]) / np.sqrt(self.class_var[c]),\n                anomaly_score)\n        \n        anomaly_score = 1 - anomaly_score\n        \n        if self.multi_scale:\n            with torch.no_grad():\n                anomaly_score = self.multi_scale(anomaly_score, prediction)\n\n        return logits, anomaly_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:51.230291Z","iopub.execute_input":"2025-06-06T20:35:51.230516Z","iopub.status.idle":"2025-06-06T20:35:51.989574Z","shell.execute_reply.started":"2025-06-06T20:35:51.230500Z","shell.execute_reply":"2025-06-06T20:35:51.988843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EnergyScore(nn.Module):\n    \"\"\"\n    taken from paper Residual Pattern Learning for Pixel-wise Out-of-Distribution Detection in Semantic Segmentation\n    \"\"\"\n    def __init__(self, segmenter, use_gaussian = True):\n        super().__init__()\n        self.segmenter = segmenter.eval().to(DEVICE)\n        self.use_gaussian = use_gaussian\n\n    @torch.no_grad()\n    def forward(self, inputs):\n\n        _, score = self.segmenter(inputs)\n        \n        anomaly_score = -(1. * torch.logsumexp(score, dim=1))\n        \n        if self.use_gaussian:\n            anomaly_score = anomaly_score.unsqueeze(0)\n            anomaly_score = transforms.GaussianBlur(7, sigma=1)(anomaly_score)\n            anomaly_score = anomaly_score.squeeze(0)\n            \n        return score, anomaly_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:49:26.250751Z","iopub.execute_input":"2025-06-06T20:49:26.251433Z","iopub.status.idle":"2025-06-06T20:49:26.258431Z","shell.execute_reply.started":"2025-06-06T20:49:26.251405Z","shell.execute_reply":"2025-06-06T20:49:26.257809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EnergyEntropyScore(nn.Module):\n    \"\"\"\n    taken from paper Open-set Anomaly Segmentation in Complex Scenarios (no code, since the paper has been published on arXiv 28/05/2025)\n    \"\"\"\n    def __init__(self, segmenter, use_gaussian = True):\n        super().__init__()\n        self.segmenter = segmenter.eval().to(DEVICE)\n        self.use_gaussian = use_gaussian\n\n    @torch.no_grad()\n    def forward(self, inputs):\n        \n        _, score = self.segmenter(inputs)\n\n        prob = torch.softmax(score, dim=1)\n        entorpy_score = -torch.sum(prob * torch.log(prob), dim=1)\n        energy = -torch.log(torch.sum(torch.exp(score),dim=1))\n\n        anomaly_score = energy*1 + entorpy_score*1\n        \n        if self.use_gaussian:\n            anomaly_score = anomaly_score.unsqueeze(0)\n            anomaly_score = transforms.GaussianBlur(7, sigma=1)(anomaly_score)\n            anomaly_score = anomaly_score.squeeze(0)\n            \n        return score, anomaly_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:49:26.769800Z","iopub.execute_input":"2025-06-06T20:49:26.770015Z","iopub.status.idle":"2025-06-06T20:49:26.776719Z","shell.execute_reply.started":"2025-06-06T20:49:26.769999Z","shell.execute_reply":"2025-06-06T20:49:26.776025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def disimilarity_entropy(logits, vanilla_logits, t=1.):\n    \"\"\"\n    loss from RPL anomaly detection\n    taken from https://github.com/yyliu01/RPL/blob/main/rpl_corocl.code/loss/PositiveEnergy.py\n    \"\"\"\n    n_prob = torch.clamp(torch.softmax(vanilla_logits, dim=1), min=1e-7)\n    a_prob = torch.clamp(torch.softmax(logits, dim=1), min=1e-7)\n\n    n_entropy = -torch.sum(n_prob * torch.log(n_prob), dim=1) / t\n    a_entropy = -torch.sum(a_prob * torch.log(a_prob), dim=1) / t\n    if torch.isnan(logits).any():\n        print(f\"logits: {logits}, a_prob: {a_prob}, a_entropy: {a_entropy}\")\n    entropy_disimilarity = F.mse_loss(input=a_entropy, target=n_entropy, reduction=\"none\")\n    assert ~torch.isnan(entropy_disimilarity).any(), print(torch.min(n_entropy), torch.max(a_entropy))\n\n    return entropy_disimilarity\n\ndef energy_loss(logits, targets, vanilla_logits, out_idx=13, t=1.):\n    \"\"\"\n    loss from RPL anomaly detection\n    taken from https://github.com/yyliu01/RPL/blob/main/rpl_corocl.code/loss/PositiveEnergy.py\n    \"\"\"\n    out_msk = (targets == out_idx)\n    void_msk = (targets == 255)\n\n    pseudo_targets = torch.argmax(vanilla_logits, dim=1)\n    outlier_msk = (out_msk | void_msk)\n    entropy_part = F.cross_entropy(input=logits, target=pseudo_targets, reduction='none')[~outlier_msk]\n    reg = disimilarity_entropy(logits=logits, vanilla_logits=vanilla_logits)[~outlier_msk]\n    if torch.sum(out_msk) > 0:\n        logits = logits.flatten(start_dim=2).permute(0, 2, 1)\n        energy_part = F.relu(torch.log(torch.sum(torch.exp(logits),dim=2))[out_msk.flatten(start_dim=1)]).mean()\n    else:\n        energy_part = torch.tensor([.0], device=targets.device)\n\n    inlier_loss = entropy_part.mean() + reg.mean()\n    outlier_loss = energy_part * 0.05 # 0.05 = energy_weight (taken from paper)\n    loss_res = inlier_loss + outlier_loss\n\n    return loss_res","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:55.217064Z","iopub.execute_input":"2025-06-06T20:35:55.217355Z","iopub.status.idle":"2025-06-06T20:35:55.450668Z","shell.execute_reply.started":"2025-06-06T20:35:55.217332Z","shell.execute_reply":"2025-06-06T20:35:55.449575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''def energy_entropy_loss(logits, targets, vanilla_logits, out_idx=13, t=1., alpha = 1):\n    \"\"\"\n    TODO: review that the implementation is correct\n    loss from Open-set Anomaly Segmentation in Complex Scenarios (code not published yet)\n    \"\"\"\n    out_msk = (targets == out_idx)\n    void_msk = (targets == 255)\n\n    pseudo_targets = torch.argmax(vanilla_logits, dim=1)\n    outlier_msk = (out_msk | void_msk)\n    entropy_part = F.cross_entropy(input=logits, target=pseudo_targets, reduction='none')[~outlier_msk]\n    reg = disimilarity_entropy(logits=logits, vanilla_logits=vanilla_logits)[~outlier_msk]\n    \n    if torch.sum(out_msk) > 0:\n        \n        prob = torch.softmax(logits, dim=1).flatten(start_dim=2).permute(0, 2, 1)\n        logits = logits.flatten(start_dim=2).permute(0, 2, 1)\n        \n        energy = torch.log(torch.sum(torch.exp(logits),dim=2))\n        entropy = -torch.sum(prob * torch.log(prob), dim=2)# / torch.log(torch.tensor(13.))\n        outlier_part = -torch.log(torch.sigmoid(-energy[out_msk.flatten(start_dim=1)])) - alpha*(entropy[out_msk.flatten(start_dim=1)])\n        inlier_part = torch.log(1 - torch.sigmoid(-energy[~out_msk.flatten(start_dim=1)])) + alpha*(entropy[~out_msk.flatten(start_dim=1)])\n        energy_entropy = outlier_part.mean() - inlier_part.mean()\n        \n    else:\n        energy_entropy = torch.tensor([.0], device=targets.device).mean()\n        \n    inlier_loss = entropy_part.mean() + reg.mean()\n    outlier_loss = energy_entropy*0.05\n    loss_res = inlier_loss + outlier_loss\n    \n    return loss_res'''\n    \ndef energy_entropy_loss(logits, targets, vanilla_logits, out_idx=13, t=1., alpha = 1):\n    \"\"\"\n    TODO: review that the implementation is correct\n    loss from Open-set Anomaly Segmentation in Complex Scenarios (code not published yet)\n    \"\"\"\n    out_msk = (targets == out_idx)\n    void_msk = (targets == 255)\n\n    pseudo_targets = torch.argmax(vanilla_logits, dim=1)\n    outlier_msk = (out_msk | void_msk)\n    entropy_part = F.cross_entropy(input=logits, target=pseudo_targets, reduction='none')[~outlier_msk]\n    reg = disimilarity_entropy(logits=logits, vanilla_logits=vanilla_logits)[~outlier_msk]\n    \n    if torch.sum(out_msk) > 0:\n        \n        eps = 1e-6  # stability constant\n\n        prob = torch.clamp(torch.softmax(logits, dim=1), min=eps).flatten(start_dim=2).permute(0, 2, 1)\n        logits = logits.flatten(start_dim=2).permute(0, 2, 1)\n        \n        energy = torch.logsumexp(logits, dim=2)\n        entropy = -torch.sum(prob * torch.log(prob), dim=2)  # no need for / log(C)\n        \n        sigmoid_energy = torch.sigmoid(-energy)\n        sigmoid_energy = torch.clamp(sigmoid_energy, min=eps, max=1 - eps)\n        \n        outlier_mask_flat = out_msk.flatten(start_dim=1)\n        inlier_mask_flat = ~outlier_mask_flat\n        \n        outlier_part = -torch.log(sigmoid_energy[outlier_mask_flat]) - alpha * entropy[outlier_mask_flat]\n        inlier_part = torch.log(1 - sigmoid_energy[inlier_mask_flat]) + alpha * entropy[inlier_mask_flat]\n        \n        energy_entropy = outlier_part.mean() - inlier_part.mean()\n        \n    else:\n        energy_entropy = torch.tensor([.0], device=targets.device).mean()\n        \n    inlier_loss = entropy_part.mean() + reg.mean()\n    outlier_loss = energy_entropy*0.05\n    loss_res = inlier_loss + outlier_loss\n    \n    return loss_res","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:35:55.451496Z","iopub.execute_input":"2025-06-06T20:35:55.451792Z","iopub.status.idle":"2025-06-06T20:35:55.471384Z","shell.execute_reply.started":"2025-06-06T20:35:55.451771Z","shell.execute_reply":"2025-06-06T20:35:55.470769Z"},"papermill":{"duration":0.034202,"end_time":"2025-05-19T17:44:33.295892","exception":false,"start_time":"2025-05-19T17:44:33.261690","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#The followings are scheduler used to balance the weights of the Region Loss and the Boundary loss during the loss computation, as written in the paper Boundary loss for highly unbalanced segmentation\n\nclass Dummy_scheduler():\n    \"inspired by Boundary loss for highly unbalanced segmentation\"\n    def __init__(self, alpha = 0.5, beta = 0.5):\n        \"\"\"\n        alpha equals to beta is the configuration leading to the best result in the paper, when alpha is constant\n        \"\"\"\n        self.alpha = alpha\n        self.beta = beta\n    def __call__(self, loss1, loss2, epoch):\n        loss_res = self.beta*loss1 + self.alpha*loss2\n        return loss_res\n\nclass Increamental_scheduler():\n    \"inspired by Boundary loss for highly unbalanced segmentation\"\n    def __init__(self, alpha_start = 0.01, beta = 0.99, n_epochs = 40, alpha_end=0.99):\n        \n        self.alpha_end = alpha_end\n        self.alpha_start = alpha_start\n        self.alpha = alpha_start\n        self.beta = beta\n        self.increment = abs(self.alpha - self.beta)/(n_epochs-10)\n        self.e = 0\n\n    def update_weights(self, epoch):\n        self.alpha = min(self.alpha_start + epoch * self.increment, self.alpha_end)\n        \n    def __call__(self, loss1, loss2, epoch):\n        loss_res = self.beta*loss1 + self.alpha*loss2\n        if self.e != epoch:\n            print(f\"before update alpha: {self.alpha}, beta: {self.beta}\")\n            self.update_weights(epoch)\n            print(f\"after update alpha: {self.alpha}, beta: {self.beta}\")\n            self.e +=1\n        return loss_res\n\nclass Rebalance_scheduler():\n    \"inspired by Boundary loss for highly unbalanced segmentation\"\n    def __init__(self, alpha_start = 0.01, n_epochs = 40, alpha_end=0.99):\n        \"\"\"\n        this scheduler is the one that leads to best results in the paper\n        \"\"\"\n        self.alpha_end = alpha_end\n        self.alpha_start = alpha_start\n        self.alpha = alpha_start #start with 0.01\n        self.beta = 1-self.alpha\n        self.increment = abs(self.alpha - self.beta)/(n_epochs-10)\n        self.e = 0\n\n    def update_weights(self, epoch):\n        self.alpha = min(self.alpha_start + epoch * self.increment, self.alpha_end)\n        self.beta = 1-self.alpha\n\n    def __call__(self, loss1, loss2, epoch):\n        loss_res = self.beta*loss1 + self.alpha*loss2\n        if self.e != epoch:\n            print(f\"before update alpha: {self.alpha}, beta: {self.beta}\")\n            self.update_weights(epoch)\n            print(f\"after update alpha: {self.alpha}, beta: {self.beta}\")\n            self.e +=1\n        return loss_res","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:35:55.472234Z","iopub.execute_input":"2025-06-06T20:35:55.472454Z","iopub.status.idle":"2025-06-06T20:35:55.488620Z","shell.execute_reply.started":"2025-06-06T20:35:55.472432Z","shell.execute_reply":"2025-06-06T20:35:55.487839Z"},"papermill":{"duration":0.031531,"end_time":"2025-05-19T17:44:33.350218","exception":false,"start_time":"2025-05-19T17:44:33.318687","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RPLDeepLab(nn.Module):\n    def __init__(self, segmenter):\n        super().__init__()\n        \n        self.encoder = self.copy_un_freeze_params(segmenter.encoder, unfreeze=False)\n        self.decoder = self.copy_un_freeze_params(segmenter.decoder, unfreeze=False)\n        \n        self.final = nn.Sequential(\n            self.copy_un_freeze_params(segmenter.decoder.block2, unfreeze=False),\n            self.copy_un_freeze_params(segmenter.segmentation_head, unfreeze=False),    \n        )\n        \n        self.atten_aspp_final = nn.Conv2d(256, 304, kernel_size=1, bias=False)\n        \n        self.residual_anomaly_block = nn.Sequential(\n            self.copy_un_freeze_params(segmenter.decoder.aspp, unfreeze=True),\n            self.copy_un_freeze_params(segmenter.decoder.up, unfreeze=True),\n            self.atten_aspp_final\n        )\n\n    def copy_un_freeze_params(self, layer: nn.Module, unfreeze: bool=True) -> nn.Module:\n        \"\"\"\n        function that create a deepcopy of a layer and unfreeze its parameters if unfreeze is True, otherwise freeze it\n\n        return: deepcopy of the layer freezed or unfreezed\n        \"\"\"\n        layer_copy = deepcopy(layer)\n        for param in layer_copy.parameters():\n            param.requires_grad = unfreeze\n        return layer_copy\n\n    def forward(self, x):\n\n        features = self.encoder(x)\n        aspp_features = self.decoder.aspp(features[-1])\n        aspp_features = self.decoder.up(aspp_features)\n        high_res_features = self.decoder.block1(features[2])\n        concat_features = torch.cat([aspp_features, high_res_features], dim=1)\n        \n        res = self.residual_anomaly_block(features[-1])\n\n        out1 = self.final(concat_features)\n        out2 = self.final(concat_features + res)\n\n        return out1, out2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:55.489340Z","iopub.execute_input":"2025-06-06T20:35:55.489577Z","iopub.status.idle":"2025-06-06T20:35:55.503739Z","shell.execute_reply.started":"2025-06-06T20:35:55.489556Z","shell.execute_reply":"2025-06-06T20:35:55.503078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class WarmUpPolyLRScheduler(_LRScheduler):\n    \"\"\"\n    adapted from RPL for pixel-wise OOD in semantica segmentation\n    \"\"\"\n    def __init__(self, optimizer, start_lr, total_iters, warmup_steps=0, lr_power=0.9, end_lr=1e-8, last_epoch=-1):\n        self.start_lr = start_lr\n        self.total_iters = total_iters\n        self.warmup_steps = warmup_steps\n        self.lr_power = lr_power\n        self.end_lr = end_lr\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        cur_iter = self.last_epoch\n        if cur_iter < self.warmup_steps:\n            lr = self.start_lr * (cur_iter / self.warmup_steps)\n        else:\n            lr = self.start_lr * ((1 - float(cur_iter) / self.total_iters) ** self.lr_power)\n            lr = np.clip(lr, a_min=self.end_lr, a_max=self.start_lr)\n\n        return [lr for _ in self.optimizer.param_groups]","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:35:55.504505Z","iopub.execute_input":"2025-06-06T20:35:55.505227Z","iopub.status.idle":"2025-06-06T20:35:55.520688Z","shell.execute_reply.started":"2025-06-06T20:35:55.505204Z","shell.execute_reply":"2025-06-06T20:35:55.520018Z"},"papermill":{"duration":0.029487,"end_time":"2025-05-19T17:44:33.402292","exception":false,"start_time":"2025-05-19T17:44:33.372805","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Trainer:\n    def __init__(self,\n                 #processor,\n                 model: nn.Module,\n                 train_loader: DataLoader,\n                 val_loader: DataLoader,\n                 cfg: dict,\n                 loss1,\n                 loss2 = None,\n                 loss_scheduler= None,\n                 device: torch.device = DEVICE,\n                 num_classes: int = len(COLORS)-1,\n                 resume_ckpt: dict = None,\n                 \n        ) -> None:\n\n        self.loss_scheduler = loss_scheduler\n        self.model_name = cfg[\"model_name\"]\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.num_classes = num_classes\n        self.patience = cfg[\"patience\"]\n        self.loss1 = loss1\n        self.loss2 = loss2\n        #self.best_model = self.model\n        \n        if resume_ckpt:\n\n            self.model = model.to(device)\n            self.model.load_state_dict(resume_ckpt['model_state_dict'])\n            \n            self.num_epochs = cfg[\"num_epochs\"] - resume_ckpt['epoch']\n            num_steps = self.num_epochs * len(train_loader)\n            \n            self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"wd\"])\n            self.optimizer.load_state_dict(resume_ckpt['optimizer_state_dict'])\n            \n            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, cfg[\"lr\"], total_steps=num_steps)\n            self.scheduler.load_state_dict(resume_ckpt['scheduler_state_dict'])\n            \n        else:\n            self.model = model.to(device)\n            self.num_epochs = cfg[\"num_epochs\"]\n            num_steps = self.num_epochs * len(train_loader)\n            self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"wd\"])\n            #self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, cfg[\"lr\"], total_steps=num_steps)\n            #self.scheduler = WarmUpPolyLRScheduler(self.optimizer, start_lr= cfg[\"lr\"], total_iters= num_steps)\n            self.scheduler = cfg[\"scheduler\"](self.optimizer, cfg[\"lr\"], num_steps)\n\n        self.mean_iou = 0.0\n        self.step = 0\n        self.best_acc = 0.0\n\n        self.ckpt_path = Path(\"ckpts\")\n        self.ckpt_path.mkdir(exist_ok=True)\n\n        wandb.init(name=self.model_name, entity=WANDB_USER, project=WANDB_PROJECT, config=cfg)\n\n    def wandb_log(self, split, loss, mean_iou, epoch):\n        \n        wandb.log({\n            f\"{split}_loss\": loss,\n            f\"{split}_mean_iou\": mean_iou,\n            }, step=(epoch))\n        \n    def train(self, verbose= False) -> None:\n        \n        for epoch in tqdm(range(self.num_epochs), desc=\"Epoch\"):\n            \n            self.model.train()\n\n            losses = []\n\n            for batch in self.train_loader: \n                    \n                imgs = batch['image'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                logits = self.model(imgs)\n                \n                if type(logits) == tuple:\n                    \n                    vanilla_logits, logits = logits\n                    if torch.isnan(logits).any():\n                        print(f\"check if an image is nan: {torch.isnan(imgs).any()}, \\ncheck if a label is nan: {torch.isnan(labels).any()}\")\n                    loss_res = self.loss1(logits=logits, targets=labels.clone(),\n                                         vanilla_logits=vanilla_logits)\n                else:\n    \n                    if not self.loss2:\n                        loss_res = self.loss1(logits, labels)\n    \n                    else:\n\n                        loss1_res = self.loss1(logits, labels)\n                        loss2_res = self.loss2(logits, labels)\n                        loss_res = self.loss_scheduler(loss1= loss1_res, loss2= loss2_res, epoch= epoch)\n                        \n                    del imgs, labels\n                            \n                losses.append(loss_res.item())\n                \n                self.optimizer.zero_grad()\n                loss_res.backward()\n                self.optimizer.step()\n                self.scheduler.step()\n            \n                del loss_res\n                \n\n            l = sum(losses) / len(losses)\n            print(f'epoch {epoch} | Training')\n            print(f'   total training loss : {l}')\n\n            print(f\"Epoch {epoch + 1}\", end = ' ')\n            self.eval(\"train\", epoch)\n            self.eval(\"val\", epoch)\n\n            if self.patience < self.step:\n                wandb.finish()\n                break\n        wandb.finish()\n\n    @torch.no_grad()\n    def eval(self, split: str, epoch: int) -> None:\n        \n        self.model.eval()\n\n        loader = self.train_loader if split == \"train\" else self.val_loader\n        \n        mean_iou = MeanIoU()\n        losses = []\n        mean_avg = []\n        std_avg = []\n        \n        for batch in loader:\n\n            imgs = batch['image'].to(self.device)\n            labels = batch['labels'].to(self.device)\n            \n            logits = self.model(imgs)\n            \n            if type(logits) == tuple:\n                \n                vanilla_logits, logits = logits\n\n                loss_res = self.loss1(logits=logits, targets=labels.clone(),\n                                     vanilla_logits=vanilla_logits)\n            else:\n\n                if not self.loss2:\n                    loss_res = self.loss1(logits, labels)\n\n                else:\n                    \n                    loss1_res = self.loss1(logits, labels)\n                    loss2_res = self.loss2(logits, labels)\n                    loss_res = self.loss_scheduler(loss1= loss1_res, loss2= loss2_res, epoch= epoch)\n\n            \n            losses.append(loss_res.item())\n\n            mean_iou.update(labels, logits)\n        \n        results = mean_iou.get_results()\n        mean_iou = results['Mean IoU']\n        \n        l = sum(losses) / len(losses)\n        print(f\"| {split.upper()} Metrics:\")\n        print(f\"  Loss: {l:.4f}\")\n        print(f\"  Mean IoU: {mean_iou:.4f}\\n\")\n\n        self.wandb_log(split= split, loss= l, mean_iou= mean_iou, epoch= epoch+1)\n\n        if (mean_iou > self.mean_iou or epoch + 1 == self.num_epochs) and split == \"val\" :\n            self.mean_iou = mean_iou\n\n            if epoch + 1 == self.num_epochs:\n                \n                torch.save(self.model.state_dict(), self.ckpt_path/f\"{self.model_name}_lastepoch.pt\")\n                torch.save({\n                    'epoch': epoch,\n                    'mean_iou': self.mean_iou,\n                    #'loss': loss,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'scheduler_state_dict': self.scheduler.state_dict(),\n                    }, self.ckpt_path / \"last_checkpoint\")\n    \n                wandb.save(self.ckpt_path/f\"{self.model_name}_lastepoch.pt\")\n                wandb.save(self.ckpt_path / \"last_checkpoint\")\n                \n                self.best_model = copy.deepcopy(self.model)\n            else:\n                \n                torch.save(self.model.state_dict(), self.ckpt_path/f\"{self.model_name}.pt\")\n                torch.save({\n                    'epoch': epoch,\n                    'mean_iou': self.mean_iou,\n                    #'loss': loss,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'scheduler_state_dict': self.scheduler.state_dict(),\n                    }, self.ckpt_path / \"best_checkpoint\")\n    \n                wandb.save(self.ckpt_path/f\"{self.model_name}.pt\")\n                wandb.save(self.ckpt_path / \"best_checkpoint\")\n                \n                self.best_model = copy.deepcopy(self.model)\n            self.step = 0\n\n        elif split == \"val\":\n            self.step += 1","metadata":{"execution":{"iopub.status.busy":"2025-06-06T20:35:55.521483Z","iopub.execute_input":"2025-06-06T20:35:55.521910Z","iopub.status.idle":"2025-06-06T20:35:55.545308Z","shell.execute_reply.started":"2025-06-06T20:35:55.521888Z","shell.execute_reply":"2025-06-06T20:35:55.544636Z"},"papermill":{"duration":0.050601,"end_time":"2025-05-19T17:44:33.475799","exception":false,"start_time":"2025-05-19T17:44:33.425198","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef compute_metrics(model, loader, aupr = True):\n\n    model.eval()\n    model.to(DEVICE)\n    mean_iou = MeanIoU()\n    mean_aupr = AUPR() if aupr is True else None\n\n    for batch in tqdm(loader):\n        imgs = batch['image'].to(DEVICE)\n        labels = batch['labels'].to(DEVICE)\n\n        pred = model(imgs)\n        \n        if type(pred) == tuple:\n            logits, anomaly_score = pred\n        else:\n            logits = pred\n\n        mean_iou.update(label_trues=labels, logits= logits)\n\n        if aupr:\n            mean_aupr.update(anomaly_score, labels)\n\n    if aupr:\n        return {\"mean_aupr\": mean_aupr.get_results(), \"mean_iou\": mean_iou.get_results()}\n    else:\n        return {\"mean_iou\": mean_iou.get_results()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:55.546158Z","iopub.execute_input":"2025-06-06T20:35:55.546418Z","iopub.status.idle":"2025-06-06T20:35:55.560730Z","shell.execute_reply.started":"2025-06-06T20:35:55.546398Z","shell.execute_reply":"2025-06-06T20:35:55.559979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef plot_anomaly_heatmap(anomaly_score, image=None, title=\"Anomaly Heatmap\"):\n    \"\"\"\n    Plots a heatmap where red = high anomaly, blue = low anomaly.\n    \n    Parameters:\n    - anomaly_score: torch.Tensor or np.array of shape [H, W]\n    - image: optional RGB image [H, W, 3] in range [0, 1] or [0, 255]\n    - title: string title for plot\n    \"\"\"\n    if isinstance(anomaly_score, torch.Tensor):\n        anomaly_score = anomaly_score.squeeze().cpu().numpy()\n    \n    # Normalize to [0, 1]\n    #anomaly_score = (anomaly_score - anomaly_score.min()) / (anomaly_score.max() - anomaly_score.min())\n\n\n    if image is not None:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n        #if isinstance(image, torch.Tensor):\n            #image = image.permute(1, 2, 0).cpu().numpy()\n        if image.max() > 1.0:\n            image = image / 255.0\n\n        \n        image1 = ax[0].imshow(anomaly_score, cmap='jet')  # 'jet': blue -> red\n        fig.colorbar(image1, ax=ax[0], label='Anomaly Score')\n        #ax[0].colorbar(label=\"Anomaly Score\")\n        ax[0].set_title(title)\n        ax[0].axis(\"off\")\n        \n        visualize_scene(image, ax[1])\n\n        \n    else:\n        \n        plt.imshow(anomaly_score, cmap='jet')\n\n        plt.colorbar(label=\"Anomaly Score\")\n        plt.title(title)\n        plt.axis(\"off\")\n        \n    plt.figure(figsize=(10, 12))\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:55.561557Z","iopub.execute_input":"2025-06-06T20:35:55.561895Z","iopub.status.idle":"2025-06-06T20:35:55.576067Z","shell.execute_reply.started":"2025-06-06T20:35:55.561874Z","shell.execute_reply":"2025-06-06T20:35:55.575348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from fvcore.nn import FlopCountAnalysis, parameter_count\ndef compute_flops_params(model):\n    input_tensor = torch.randn(8, 3, 512, 896).to(DEVICE)\n    flops = FlopCountAnalysis(model, input_tensor)\n    params = parameter_count(model)\n    \n    print(\"FLOPs (GFLOPs):\", f\"{flops.total() / 1e9:.2f}\")\n    print(\"Parameters (Millions):\", f\"{params[''] / 1e6:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:55.576775Z","iopub.execute_input":"2025-06-06T20:35:55.576957Z","iopub.status.idle":"2025-06-06T20:35:55.655362Z","shell.execute_reply.started":"2025-06-06T20:35:55.576943Z","shell.execute_reply":"2025-06-06T20:35:55.654698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Segmenter(nn.Module):\n    def __init__(self, encoder_name, encoder_weights = \"imagenet\", activation= None, num_classes= 13):\n        super().__init__()\n\n        self.model = smp.DeepLabV3Plus(encoder_name=encoder_name, \n                                       encoder_weights=encoder_weights, \n                                        classes=num_classes,\n                                        activation=activation).to(DEVICE)\n\n    def forward(self, inputs):\n        \n        logits = self.model(inputs)\n        return logits\n        \n    def wandb_load_weights(self, run_id, model_name):\n\n        api = wandb.Api()\n        run = api.run(run_id)\n        \n        files = run.files()\n        for f in files:\n            if f.name.startswith(\"ckpts/\"):\n                f.download(replace=True)\n            \n        weight_path = f\"/kaggle/working/ckpts/{model_name}.pt\"\n        model_weigths = torch.load(weight_path, weights_only=True)\n        self.load_state_dict(model_weigths)\n\n    def load_weights(self, weight_path):\n\n        model_weigths = torch.load(weight_path, weights_only=True)\n        self.model.load_state_dict(model_weigths)\n    \n    @torch.no_grad()\n    def predict(self, inputs):\n        \n        self.model.eval()\n        pred = self.model(inputs.unsqueeze(0).to(DEVICE))\n        pred = pred.squeeze(0)\n        pred = torch.argmax(pred, dim = 0)\n        return pred.cpu()\n\n    def get_flops(self, input_shape):\n        print(summary(self.model, input_shape))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:55.656138Z","iopub.execute_input":"2025-06-06T20:35:55.656447Z","iopub.status.idle":"2025-06-06T20:35:55.663301Z","shell.execute_reply.started":"2025-06-06T20:35:55.656356Z","shell.execute_reply":"2025-06-06T20:35:55.662642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def wandb_load_weights(model, run_id, model_name):\n\n    api = wandb.Api()\n    run = api.run(run_id)\n    \n    files = run.files()\n    for f in files:\n        if f.name.startswith(\"ckpts/\"):\n            f.download(replace=True)\n        \n    weight_path = f\"/kaggle/working/ckpts/{model_name}.pt\"\n    model_weigths = torch.load(weight_path, weights_only=True)\n    model.load_state_dict(model_weigths)\n\ndef load_weights(model, weight_path):\n\n    model_weigths = torch.load(weight_path, weights_only=True)\n    model.load_state_dict(model_weigths)\n\n@torch.no_grad()\ndef predict(model, inputs):\n    \n    model.eval()\n    pred = model(inputs.unsqueeze(0).to(DEVICE))\n    pred = pred.squeeze(0)\n    pred = torch.argmax(pred, dim = 0)\n    return pred.cpu()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:55.663982Z","iopub.execute_input":"2025-06-06T20:35:55.664257Z","iopub.status.idle":"2025-06-06T20:35:55.679183Z","shell.execute_reply.started":"2025-06-06T20:35:55.664238Z","shell.execute_reply":"2025-06-06T20:35:55.678490Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Introduction\n## 1.1 Overall Approach\n\nThe approach used to solve the problem follows the idea of many paper that can be found in the literature:\n- train a segmenter model, which solves the inlier segmentation without considering the anomaly part\n- add on top of the segmentation model an anomaly detector\nFor the inlier segmentation task the choosen architecture has been DeepLabV3plus, a well-know architecure in the literature developed for segmentation purposes, for the feature extractor the idea was to compare the performances of two lightweight models:\n- EfficientNet-b0\n- MobileNetV2\nAt the end the choosen architecure has been EfficientNet-b0, which allows to obtain best performances without increasing too much the complexity of the model and the execution time.\n\nFor the anomaly detection part, the implemented approach use Residual Pattern Learning idea presented in the paper: Residual Pattern Learning for Pixel-wise Out-of-Distribution Detection in Semantic Segmentation, the peak performance has been reached with the new loss presented in a recent publication: Open-set Anomaly Segmentation in Complex Scenarios, Song Xia et al. \n\nAll the experiments have been performed using Kaggle platform, the choice of lighter architecture, which clearly does not allow to reach top score performances, is motivated by the strong limit in computational resources. Nonethless, i have been able to reach performances in line with the results presented in papers.","metadata":{}},{"cell_type":"markdown","source":"## 1.2 Metrics and Evaluation\n\nThe architecures have been compared, as required, using two metrics:\n- MeanIoU, computed according to the definition given during the lectures, using the implementation provided in the repository of DeepLabV3+. It has been used to measure the inlier segmentation of the models\n- AUPR, computed using the scikit-learn metrics `average_precision_score`, by averaging over each individual samples.","metadata":{}},{"cell_type":"markdown","source":"# 2. Dataset\nThe Dataset used is StreetHazards, one thing that is noticeable among labels is the followings:\n- BACKGROUND, which should represent mainly the sky, contains inside it self also other obects;\n- OTHERS, as the name says, represent a multitude of objects.\nthis makes the task even more complicated, since the model could be tricky.","metadata":{}},{"cell_type":"code","source":"shape_resize = (512, 896)\n#shape_resize = (688, 688)\n\nmean_imagenet, std_imagenet = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n\n#to apply to both image and labels before tensorise them\nspatial_transforms = transforms.v2.Compose([\n    #transforms.v2.RandomCrop(shape_resize),\n    transforms.v2.RandomHorizontalFlip(),\n])\n\n#to be applied only to images after tensorise them\nimages_only_transforms = transforms.Compose([\n    transforms.Normalize(mean = mean_imagenet, std = std_imagenet),\n    #transforms.Normalize(mean = mean_streethazards, std = std_streethazards),\n    #transforms.RandomErasing(scale=(0.02, 0.15))\n])\n\n#only apply resize, to_tensor and normalization (computed on train)\nval_test_transforms = transforms.Normalize(\n    #mean = mean_streethazards, std = std_streethazards\n    mean = mean_imagenet, std = std_imagenet\n)\n\ntrain_dataset = StreetHazardsDataset(\n    odgt_file=\"/kaggle/input/streethazards_train/train/train.odgt\",\n    image_resize = shape_resize,\n    spatial_transforms=spatial_transforms,\n    images_only_transforms=images_only_transforms\n)\n\nval_dataset = StreetHazardsDataset(\n    odgt_file=\"/kaggle/input/streethazards_train/train/validation.odgt\",\n    image_resize = shape_resize,\n    spatial_transforms=None,\n    images_only_transforms=val_test_transforms\n)\n\ntest_dataset = StreetHazardsDataset(\n    odgt_file=\"/kaggle/input/streethazards_test/test/test.odgt\",\n    image_resize = shape_resize,\n    spatial_transforms=None,\n    images_only_transforms=val_test_transforms\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:55.679910Z","iopub.execute_input":"2025-06-06T20:35:55.680124Z","iopub.status.idle":"2025-06-06T20:35:55.840034Z","shell.execute_reply.started":"2025-06-06T20:35:55.680107Z","shell.execute_reply":"2025-06-06T20:35:55.839420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(10, 12))\nidx = 2\nimg, label = train_dataset[idx].values()\nvisualize_scene(img, axs[0])\nvisualize_annotation(label, axs[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:55.840625Z","iopub.execute_input":"2025-06-06T20:35:55.840823Z","iopub.status.idle":"2025-06-06T20:35:56.415125Z","shell.execute_reply.started":"2025-06-06T20:35:55.840809Z","shell.execute_reply":"2025-06-06T20:35:56.414247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dl = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\nval_dl = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\ntest_dl = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:56.415989Z","iopub.execute_input":"2025-06-06T20:35:56.416193Z","iopub.status.idle":"2025-06-06T20:35:56.420623Z","shell.execute_reply.started":"2025-06-06T20:35:56.416177Z","shell.execute_reply":"2025-06-06T20:35:56.419992Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.2 Data Exploration\nIt can be observed that there are sequences of images, correlated by spatial dependencies. Since the prediction is assumed to be made to single images, it has been decided to shuffle the train dataset. This should avoid any leakage of information during training that could, illegitimately, boost model performances","metadata":{}},{"cell_type":"code","source":"'''fig, axs = plt.subplots(1, 3, figsize=(10, 12))\ncompute_class_frequency(dataset = train_dataset, num_classes = len(COLORS)-1, plot_frequencies = True, ax= axs[0]);\ncompute_class_frequency(dataset = val_dataset, num_classes = len(COLORS)-1, plot_frequencies = True, ax= axs[1]);\ncompute_class_frequency(dataset = test_dataset, num_classes = len(COLORS), plot_frequencies = True, ax= axs[2]);\nplt.show()'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:56.421288Z","iopub.execute_input":"2025-06-06T20:35:56.421572Z","iopub.status.idle":"2025-06-06T20:35:56.434026Z","shell.execute_reply.started":"2025-06-06T20:35:56.421547Z","shell.execute_reply":"2025-06-06T20:35:56.433422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.3 Anomalies dataset\n\nThe Residual Pattern Learning architecture required an additional training over a frozen segmenter using dataset with anomalies, and the anomaly pixels are only present in the test set, both train and validation for that part will be enanched with anomlies take from PascalVoc. Most of the papers uses for anomaly injection COCO dataset, but due to lack of memory in the kaggle run time  directory PascalVoc has been choosen as lighter alternative. \nAs steated in Open-set Anomaly Segmentation in Complex Scenarios, Song Xia et al. this copy-paste is not the best solutions, since anomalies are placed in a randomly, but the usage of a Diffusion Model in my case would have been too computationally expensive.","metadata":{}},{"cell_type":"code","source":"mix_train = MixDataset(inlier_dataset= StreetHazardsDataset(odgt_file=\"/kaggle/input/streethazards_train/train/train.odgt\",\n                                                            image_resize = shape_resize,\n                                                            spatial_transforms=None,\n                                                            images_only_transforms=None), \n                       outlier_dataset= voc_train, \n                       images_only_transforms= images_only_transforms)\n\nmix_val = MixDataset(inlier_dataset= StreetHazardsDataset(odgt_file=\"/kaggle/input/streethazards_train/train/validation.odgt\",\n                                                          image_resize = shape_resize,\n                                                          spatial_transforms=None,\n                                                          images_only_transforms=None), \n                     outlier_dataset= voc_val, \n                     images_only_transforms= images_only_transforms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:56.434911Z","iopub.execute_input":"2025-06-06T20:35:56.435164Z","iopub.status.idle":"2025-06-06T20:35:56.521468Z","shell.execute_reply.started":"2025-06-06T20:35:56.435143Z","shell.execute_reply":"2025-06-06T20:35:56.520804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mix_train_dl = DataLoader(mix_train, batch_size=8, shuffle=True, num_workers=2)\nmix_val_dl = DataLoader(mix_val, batch_size=8, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:56.522197Z","iopub.execute_input":"2025-06-06T20:35:56.522442Z","iopub.status.idle":"2025-06-06T20:35:56.526319Z","shell.execute_reply.started":"2025-06-06T20:35:56.522427Z","shell.execute_reply":"2025-06-06T20:35:56.525548Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3 Inlier Segmentation\n\nIn this section i will show the results of the inlier segmentation and the ablation study which has been made to arrive at this results","metadata":{}},{"cell_type":"code","source":"efficientnetb0 = smp.DeepLabV3Plus(encoder_name='efficientnet-b0', \n                                   encoder_weights='imagenet', \n                                   classes=13,\n                                   activation=None).to(DEVICE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/8yuhosdd', model_name= 'CrossEntropyDice_HorizontalFlip_lower_wd') #wd = 1e-5\naupr = 0\niou = 0\nfor i in range(3):\n    res = compute_metrics(MaxLogit(efficientnetb0), mix_val_dl)\n    aupr += res['mean_aupr']\n    iou += res['mean_iou']['Mean IoU']\n\nprint('mean_aupr: ', aupr/3)\nprint('mean_iou: ', iou/3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/6rpqmc22', model_name= 'CrossEntropy_Dice_HorizontalFlip_no_weight_decay') #wd = 0\n\naupr = 0\niou = 0\nfor i in range(3):\n    res = compute_metrics(MaxLogit(efficientnetb0), mix_val_dl)\n    aupr += res['mean_aupr']\n    iou += res['mean_iou']['Mean IoU']\n\nprint('mean_aupr: ', aupr/3)\nprint('mean_iou: ', iou/3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/baeu3kou', model_name= 'CrossEntropyDice_HorizontalFlip_wd_1eminus4') #wd = 1e-4\n\naupr = 0\niou = 0\nfor i in range(3):\n    res = compute_metrics(MaxLogit(efficientnetb0), mix_val_dl)\n    aupr += res['mean_aupr']\n    iou += res['mean_iou']['Mean IoU']\n\nprint('mean_aupr: ', aupr/3)\nprint('mean_iou: ', iou/3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/2txwhmej', model_name= 'CrossEntropyDice_HorizontalFlip_higher_wd') #ed = 1e-1\n\nprint(compute_metrics(MaxLogit(efficientnetb0), mix_val_dl))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/4h6rbhuw', model_name= 'CrossEntropyDice_HorizontalFlip_wd_1eminus2')#wd = 1e-2\nprint(compute_metrics(MaxLogit(efficientnetb0), mix_val_dl))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/z2x53n4i', model_name= 'CrossEntropyDice_with_HorizontalFlip') #wd = 1e-3\n\nprint(compute_metrics(MaxLogit(efficientnetb0), mix_val_dl))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/yxjodd6i', model_name= 'CrossEntropyFocal_HorizontalFlip') #wd = 1e-3\n\nprint(compute_metrics(MaxLogit(efficientnetb0), mix_val_dl))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/a9ivthlz', model_name= 'CrossEntropyJaccard_HorizontalFlip') #wd = 1e-3\n\nprint(compute_metrics(MaxLogit(efficientnetb0), mix_val_dl))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/ndoh4hkz', model_name= 'CrossEntropyLovasz_HorizontalFlip') #wd = 1e-3\n\nprint(compute_metrics(MaxLogit(efficientnetb0), mix_val_dl))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"err","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.1 mobilenetv2 vs efficientnet-b0\n- wd\n- lr\n- the loss used has been cross entropy","metadata":{}},{"cell_type":"code","source":"'''cfg_original = {\n    \"num_epochs\" : 40,\n    \"lr\": 2e-4,\n    \"wd\": 0.001,\n    \"patience\": 1000,\n    \"model_name\": \"higher_lr_mobilenetv2\",\n    #scheduler = torch.optim.lr_scheduler.OneCycleLR\n}'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:56.527073Z","iopub.execute_input":"2025-06-06T20:35:56.527312Z","iopub.status.idle":"2025-06-06T20:35:56.537915Z","shell.execute_reply.started":"2025-06-06T20:35:56.527288Z","shell.execute_reply":"2025-06-06T20:35:56.537356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#efficientnetb0 = Segmenter(encoder_name= 'efficientnet-b0')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:56.538533Z","iopub.execute_input":"2025-06-06T20:35:56.538863Z","iopub.status.idle":"2025-06-06T20:35:56.548662Z","shell.execute_reply.started":"2025-06-06T20:35:56.538840Z","shell.execute_reply":"2025-06-06T20:35:56.548050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cfg = {\n    \"num_epochs\" : 40,\n    \"lr\": 2e-4,\n    \"wd\": 0.001,\n    \"patience\": 1000,\n    \"model_name\": \"CrossEntropy_Dice_HorizontalFlip_mobilenetv2\",\n    \"scheduler\": torch.optim.lr_scheduler.OneCycleLR\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:57.558157Z","iopub.execute_input":"2025-06-06T20:35:57.558482Z","iopub.status.idle":"2025-06-06T20:35:57.562273Z","shell.execute_reply.started":"2025-06-06T20:35:57.558452Z","shell.execute_reply":"2025-06-06T20:35:57.561626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''trainer = Trainer(\n    model= efficientnetb0,\n    train_loader= train_dl,\n    val_loader= val_dl,\n    loss1 = F.cross_entropy,\n    loss2 = losses.DiceLoss(mode = 'multiclass'),\n    loss_scheduler = Dummy_scheduler(),\n    cfg= cfg\n)\n\n#trainer.train()'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:35:57.563014Z","iopub.execute_input":"2025-06-06T20:35:57.563245Z","iopub.status.idle":"2025-06-06T20:36:04.633556Z","shell.execute_reply.started":"2025-06-06T20:35:57.563227Z","shell.execute_reply":"2025-06-06T20:36:04.633042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/2txwhmej', model_name= 'CrossEntropyDice_HorizontalFlip_higher_wd') #0.1\n\nprint(compute_metrics(MaxLogit(efficientnetb0), mix_val_dl))'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:36:04.634259Z","iopub.execute_input":"2025-06-06T20:36:04.634446Z","iopub.status.idle":"2025-06-06T20:36:04.639664Z","shell.execute_reply.started":"2025-06-06T20:36:04.634431Z","shell.execute_reply":"2025-06-06T20:36:04.638976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/z2x53n4i', model_name= 'CrossEntropyDice_with_HorizontalFlip') #wd= 0.001\n\nprint(compute_metrics(MaxLogit(efficientnetb0), mix_val_dl))'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:36:04.640410Z","iopub.execute_input":"2025-06-06T20:36:04.641156Z","iopub.status.idle":"2025-06-06T20:36:04.650392Z","shell.execute_reply.started":"2025-06-06T20:36:04.641139Z","shell.execute_reply":"2025-06-06T20:36:04.649862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/8yuhosdd', model_name= 'CrossEntropyDice_HorizontalFlip_lower_wd') #0.00001\n\nprint(compute_metrics(MaxLogit(efficientnetb0), mix_val_dl))'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:36:04.651160Z","iopub.execute_input":"2025-06-06T20:36:04.651890Z","iopub.status.idle":"2025-06-06T20:36:04.661170Z","shell.execute_reply.started":"2025-06-06T20:36:04.651865Z","shell.execute_reply":"2025-06-06T20:36:04.660490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/6rpqmc22', model_name= 'CrossEntropy_Dice_HorizontalFlip_no_weight_decay')\n\n#print(compute_metrics(MaxLogit(efficientnetb0), mix_val_dl))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:36:04.661960Z","iopub.execute_input":"2025-06-06T20:36:04.662633Z","iopub.status.idle":"2025-06-06T20:36:20.909213Z","shell.execute_reply.started":"2025-06-06T20:36:04.662610Z","shell.execute_reply":"2025-06-06T20:36:20.908699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rpl_cfg = {\n    \"num_epochs\" : 30,\n    \"lr\": 7.5e-5,\n    \"wd\": 1e-4,\n    \"patience\": 1000,\n    \"model_name\": \"trial_energy_entropy_loss\",\n    \"scheduler\": WarmUpPolyLRScheduler\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:36:20.910037Z","iopub.execute_input":"2025-06-06T20:36:20.910533Z","iopub.status.idle":"2025-06-06T20:36:20.914958Z","shell.execute_reply.started":"2025-06-06T20:36:20.910507Z","shell.execute_reply":"2025-06-06T20:36:20.914343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rpl = RPLDeepLab(efficientnetb0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T20:36:20.915746Z","iopub.execute_input":"2025-06-06T20:36:20.916355Z","iopub.status.idle":"2025-06-06T20:36:20.977119Z","shell.execute_reply.started":"2025-06-06T20:36:20.916331Z","shell.execute_reply":"2025-06-06T20:36:20.976372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    model= rpl,\n    train_loader= mix_train_dl,\n    val_loader= mix_val_dl,\n    loss1 = energy_loss,\n    cfg= rpl_cfg\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"load_weights(model= rpl, weight_path= f'/kaggle/working/ckpts/{rpl_cfg[\"model_name\"]}.pt')\nprint(compute_metrics(EnergyEntropyScore(rpl), test_dl))\nprint(compute_metrics(EnergyScore(rpl), test_dl))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"load_weights(model= rpl, weight_path= f'/kaggle/working/ckpts/{rpl_cfg[\"model_name\"]}_lastepoch.pt')\nprint(compute_metrics(EnergyEntropyScore(rpl), test_dl))\nprint(compute_metrics(EnergyScore(rpl), test_dl))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#energy loss rpl\nwandb_load_weights(model= rpl, run_id = 'chri-project/ML4CV--assignment/5l6w1nue', model_name= 'rpl_efficientnet_ce_dice_horizontal_flip')\n\nprint(compute_metrics(EnergyEntropyScore(rpl), test_dl))\nprint(compute_metrics(EnergyScore(rpl), test_dl))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"err","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### other losses","metadata":{}},{"cell_type":"markdown","source":"688, 688 has the same flops consumption at training time of the other size, but it is squared","metadata":{}},{"cell_type":"markdown","source":"### Dice","metadata":{}},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/n6nxoni1', model_name= 'CrossentropyDice_with_HorizontalFlip_RandomCrop')\n\naupr = 0\niou = 0\nfor i in range(3):\n    res = compute_metrics(MaxLogit(efficientnetb0), mix_val_dl)\n    aupr += res['mean_aupr']\n    iou += res['mean_iou']['Mean IoU']\n\nprint('mean_aupr: ', aupr/3)\nprint('mean_aupr: ', iou/3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:09:22.012859Z","iopub.execute_input":"2025-06-04T15:09:22.013133Z","iopub.status.idle":"2025-06-04T15:19:10.663527Z","shell.execute_reply.started":"2025-06-04T15:09:22.013110Z","shell.execute_reply":"2025-06-04T15:19:10.662664Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Lovasz","metadata":{}},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/dj7yebxb', model_name= 'CrossEntropyLovasz_HorizontalFlip_RandomCrop')\naupr = 0\niou = 0\nfor i in range(3):\n    res = compute_metrics(MaxLogit(efficientnetb0), mix_val_dl)\n    aupr += res['mean_aupr']\n    iou += res['mean_iou']['Mean IoU']\n\nprint('mean_aupr: ', aupr/3)\nprint('mean_aupr: ', iou/3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:50:57.891988Z","iopub.execute_input":"2025-06-04T13:50:57.892240Z","iopub.status.idle":"2025-06-04T14:00:44.978598Z","shell.execute_reply.started":"2025-06-04T13:50:57.892218Z","shell.execute_reply":"2025-06-04T14:00:44.977632Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Jaccard","metadata":{}},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/eglcncxi', model_name= 'CrossEntropyJaccard_HorizontalFlip_RandomCrop')\naupr = 0\niou = 0\nfor i in range(3):\n    res = compute_metrics(MaxLogit(efficientnetb0), mix_val_dl)\n    aupr += res['mean_aupr']\n    iou += res['mean_iou']['Mean IoU']\n\nprint('mean_aupr: ', aupr/3)\nprint('mean_aupr: ', iou/3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:00:44.979680Z","iopub.execute_input":"2025-06-04T14:00:44.979939Z","iopub.status.idle":"2025-06-04T14:10:37.608086Z","shell.execute_reply.started":"2025-06-04T14:00:44.979890Z","shell.execute_reply":"2025-06-04T14:10:37.607128Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Focal","metadata":{}},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/56rxyhvp', model_name= 'CrossEntropyFocal_HorizontalFlip_RandomCrop')\naupr = 0\niou = 0\nfor i in range(3):\n    res = compute_metrics(MaxLogit(efficientnetb0), mix_val_dl)\n    aupr += res['mean_aupr']\n    iou += res['mean_iou']['Mean IoU']\n\nprint('mean_aupr: ', aupr/3)\nprint('mean_aupr: ', iou/3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:10:37.609659Z","iopub.execute_input":"2025-06-04T14:10:37.609923Z","iopub.status.idle":"2025-06-04T14:20:19.489435Z","shell.execute_reply.started":"2025-06-04T14:10:37.609878Z","shell.execute_reply":"2025-06-04T14:20:19.488552Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### higher weight decay","metadata":{}},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/zzgcb1sj', model_name= 'Higher_wd_CE_Dice_HorizontalFlip_RandomCrop')\naupr = 0\niou = 0\nfor i in range(3):\n    res = compute_metrics(MaxLogit(efficientnetb0), mix_val_dl)\n    aupr += res['mean_aupr']\n    iou += res['mean_iou']['Mean IoU']\n\nprint('mean_aupr: ', aupr/3)\nprint('mean_iou: ', iou/3)\nprint((aupr/3 + iou/3)/2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:59:42.979925Z","iopub.execute_input":"2025-06-04T15:59:42.980225Z","iopub.status.idle":"2025-06-04T16:09:32.859268Z","shell.execute_reply.started":"2025-06-04T15:59:42.980201Z","shell.execute_reply":"2025-06-04T16:09:32.857699Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### lower weight decay","metadata":{}},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/l1rcyoa5', model_name= 'Lower_wd_CE_Dice_HorizontalFlip_RandomCrop')\naupr = 0\niou = 0\nfor i in range(3):\n    res = compute_metrics(MaxLogit(efficientnetb0), mix_val_dl)\n    aupr += res['mean_aupr']\n    iou += res['mean_iou']['Mean IoU']\n\nprint('mean_aupr: ', aupr/3)\nprint('mean_iou: ', iou/3)\nprint((aupr/3 + iou/3)/2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T16:09:32.860822Z","iopub.execute_input":"2025-06-04T16:09:32.861166Z","iopub.status.idle":"2025-06-04T16:19:22.021648Z","shell.execute_reply.started":"2025-06-04T16:09:32.861144Z","shell.execute_reply":"2025-06-04T16:19:22.020347Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### no normalization","metadata":{}},{"cell_type":"code","source":"no_norm_val_dataset = StreetHazardsDataset(\n    odgt_file=\"/kaggle/input/streethazards_train/train/validation.odgt\",\n    image_resize = shape_resize,\n    spatial_transforms=None,\n    images_only_transforms=None\n)\n\nno_norm_mix_val = MixDataset(inlier_dataset= no_norm_val_dataset, outlier_dataset= voc_val, images_only_transforms= None)\nno_norm_mix_val_dl = DataLoader(no_norm_mix_val, batch_size=8, shuffle=False, num_workers=2)\n\nwandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/x068ojxv', model_name= 'CrossEntropyDice_HorizontalFlip_RandomCrop_without_Normalization')\naupr = 0\niou = 0\nfor i in range(3):\n    res = compute_metrics(MaxLogit(efficientnetb0), no_norm_mix_val_dl)\n    aupr += res['mean_aupr']\n    iou += res['mean_iou']['Mean IoU']\n\nprint('mean_aupr: ', aupr/3)\nprint('mean_iou: ', iou/3)\nprint((aupr/3 + iou/3*100)/2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:39:15.811497Z","iopub.execute_input":"2025-06-04T15:39:15.811791Z","iopub.status.idle":"2025-06-04T15:49:05.641431Z","shell.execute_reply.started":"2025-06-04T15:39:15.811766Z","shell.execute_reply":"2025-06-04T15:49:05.640480Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### only random flip","metadata":{}},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/z2x53n4i', model_name= 'CrossEntropyDice_with_HorizontalFlip')\naupr = 0\niou = 0\nfor i in range(3):\n    res = compute_metrics(MaxLogit(efficientnetb0), mix_val_dl)\n    aupr += res['mean_aupr']\n    iou += res['mean_iou']['Mean IoU']\n\nprint('mean_aupr: ', aupr/3)\nprint('mean_iou: ', iou/3)\nprint((aupr/3 + iou/3)/2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:49:46.239411Z","iopub.execute_input":"2025-06-04T14:49:46.239684Z","iopub.status.idle":"2025-06-04T14:59:36.205994Z","shell.execute_reply.started":"2025-06-04T14:49:46.239662Z","shell.execute_reply":"2025-06-04T14:59:36.205114Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### no augmentations","metadata":{}},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/aqdwcrnh', model_name= 'DiceLoss_and_cross_entropy_without_normalization')\naupr = 0\niou = 0\nfor i in range(3):\n    res = compute_metrics(MaxLogit(efficientnetb0), mix_val_dl)\n    aupr += res['mean_aupr']\n    iou += res['mean_iou']['Mean IoU']\n\nprint('mean_aupr: ', aupr/3)\nprint('mean_iou: ', iou/3)\nprint((aupr/3 + iou/3)/2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:59:36.207055Z","iopub.execute_input":"2025-06-04T14:59:36.207328Z","iopub.status.idle":"2025-06-04T15:09:22.011830Z","shell.execute_reply.started":"2025-06-04T14:59:36.207303Z","shell.execute_reply":"2025-06-04T15:09:22.010976Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### square images","metadata":{}},{"cell_type":"code","source":"wandb_load_weights(model= efficientnetb0, run_id = 'chri-project/ML4CV--assignment/53vpt4ew', model_name= 'CrossEntropyDice_HorizontalFlip_RandomCrop_square_images')\naupr = 0\niou = 0\nfor i in range(3):\n    res = compute_metrics(MaxLogit(efficientnetb0), mix_val_dl)\n    aupr += res['mean_aupr']\n    iou += res['mean_iou']['Mean IoU']\n\nprint('mean_aupr: ', aupr/3)\nprint('mean_iou: ', iou/3)\nprint((aupr/3 + iou/3)/2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:49:05.642842Z","iopub.execute_input":"2025-06-04T15:49:05.643107Z","iopub.status.idle":"2025-06-04T15:58:53.486865Z","shell.execute_reply.started":"2025-06-04T15:49:05.643085Z","shell.execute_reply":"2025-06-04T15:58:53.485951Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.1.1 Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Image resizing\nUsing lower image size, make the image lossing small details as shown by lower Mean IoU. The best choice would have been to keep the image as it is, but due to low computational power i have had to resize images. Furthermore i have decided to keep the same relationship between width and height since this increases a little the performances instead of making the image squares.","metadata":{}},{"cell_type":"markdown","source":"### Data normalization\nThe images have been normalized according to ImageNet mean and std, since the models have pretrained weights on imagenet. Indeed, removing data normalization, or changing it using mean and std computed on the StreetHazards train set worse both Mean IoU and AUPR.","metadata":{}},{"cell_type":"markdown","source":"## 3.1.1 hyperparameter tuning","metadata":{}},{"cell_type":"markdown","source":"### Learning rate","metadata":{}},{"cell_type":"markdown","source":"### Weight decay","metadata":{}},{"cell_type":"markdown","source":"## 3.1.2 data augmentation\n\nData augmentation with data augmentation i found that that using both Horizontal flips and Random Crop improves not only MeanIoU, but also AUPR curve","metadata":{}},{"cell_type":"markdown","source":"## 3.2 losses\nFollowing the expoloration on Cityscapes datset of the paper: LOSS FUNCTIONS IN THE ERA OF SEMANTIC SEGMENTATION: A SURVEY AND OUTLOOK, I have experimented the losses which does not requires hyperparameter tuning in combination with Cross Entropy, using a so called Dummy Scheduler, which combine the $L_{CE}$ and $L_{var}$. The final loss has been $L_{total}$ = 0.5 * $L_{CE}$ + 0.5 * $L_{var}$.","metadata":{}},{"cell_type":"markdown","source":"## 3.3 Results","metadata":{}},{"cell_type":"markdown","source":"# 4 Anomaly segmentation\n\nIn this section I will shoe the results of the anomaly segmentation and the ablation study made to arrive at this results","metadata":{}},{"cell_type":"markdown","source":"# 5 Final Results\n","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Quantitative results","metadata":{}},{"cell_type":"markdown","source":"## 5.2 Qualitative results","metadata":{}},{"cell_type":"markdown","source":"#### try to freeze some layer (?)","metadata":{"papermill":{"duration":0.023176,"end_time":"2025-05-19T17:44:37.336464","exception":false,"start_time":"2025-05-19T17:44:37.313288","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# RPL implementation","metadata":{"papermill":{"duration":0.023378,"end_time":"2025-05-19T17:44:38.091444","exception":false,"start_time":"2025-05-19T17:44:38.068066","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model = get_deeplab_model(encoder_name=\"mobilenet_v2\", encoder_weights = \"imagenet\", activation= None, num_classes= 13)\nmn = load_model_weights(model = model,\n                               run_id = \"chri-project/ML4CV--assignment/mgg66q6x\", \n                               model_name= \"cross_entropy_mobilenetv2_without_normalization\")","metadata":{"papermill":{"duration":0.029703,"end_time":"2025-05-19T17:44:40.848423","exception":false,"start_time":"2025-05-19T17:44:40.818720","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T20:34:25.703722Z","iopub.status.idle":"2025-06-03T20:34:25.703987Z","shell.execute_reply.started":"2025-06-03T20:34:25.703864Z","shell.execute_reply":"2025-06-03T20:34:25.703876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rpl = RPLDeepLab(mn)\nsummary(rpl, input_size= (8, 3, 512, 896))","metadata":{"papermill":{"duration":0.744069,"end_time":"2025-05-19T17:44:41.625385","exception":false,"start_time":"2025-05-19T17:44:40.881316","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T20:34:25.704946Z","iopub.status.idle":"2025-06-03T20:34:25.705148Z","shell.execute_reply.started":"2025-06-03T20:34:25.705055Z","shell.execute_reply":"2025-06-03T20:34:25.705063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"rpl_energy_entropy_loss_mobilenetv2_crossentropy\"\n\nrpl_cfg = {\n    \"num_epochs\" : 40,\n    \"lr\": 5e-5,\n    \"wd\": 0.001,\n    \"patience\": 1000,\n    \"segmenter\": \"mobilenetv2_crossentropy\",\n    \"run_name\": model_name,\n}","metadata":{"papermill":{"duration":0.057461,"end_time":"2025-05-19T17:46:40.566950","exception":false,"start_time":"2025-05-19T17:46:40.509489","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T20:34:25.706659Z","iopub.status.idle":"2025-06-03T20:34:25.706971Z","shell.execute_reply.started":"2025-06-03T20:34:25.706814Z","shell.execute_reply":"2025-06-03T20:34:25.706828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rpl_trainer = Trainer(\n    model= rpl,\n    train_loader= mix_train_dl,\n    val_loader= mix_val_dl ,\n    #loss1 = energy_loss,\n    loss1 = energy_entropy_loss,\n    device= DEVICE,\n    num_classes = len(COLORS),\n    model_name = model_name,\n    cfg= rpl_cfg,\n    scheduler = WarmUpPolyLRScheduler\n    #resume_ckpt = torch.load(resume_ckpt)\n)\n\nrpl_trainer.train()","metadata":{"papermill":{"duration":42988.026602,"end_time":"2025-05-20T05:43:08.649813","exception":false,"start_time":"2025-05-19T17:46:40.623211","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T20:34:25.707912Z","iopub.status.idle":"2025-06-03T20:34:25.708214Z","shell.execute_reply.started":"2025-06-03T20:34:25.708057Z","shell.execute_reply":"2025-06-03T20:34:25.708071Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# JUMP HERE","metadata":{"papermill":{"duration":0.064583,"end_time":"2025-05-20T05:43:09.598429","exception":false,"start_time":"2025-05-20T05:43:09.533846","status":"completed"},"tags":[]}},{"cell_type":"code","source":"run_id_rpl = \"chri-project/ML4CV--assignment/a17kpqqs\"\nrpl_model_name = \"rpl_training_energy_loss\"\n\nrpl = load_model_weights(run_id_rpl, rpl_model_name, rpl)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T20:34:25.708889Z","iopub.status.idle":"2025-06-03T20:34:25.709134Z","shell.execute_reply.started":"2025-06-03T20:34:25.709008Z","shell.execute_reply":"2025-06-03T20:34:25.709020Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### rpl with energy loss results (RPL original paper)","metadata":{}},{"cell_type":"code","source":"run_id_rpl = \"chri-project/ML4CV--assignment/ht31svvx\"\nrpl_model_name = \"rpl_energy_entropy_loss_over_tversky\"\ndirectory = resume_run(run_id_rpl, rpl_model_name)\nmodel_weights_path = torch.load(directory, weights_only=True)\nrpl.load_state_dict(model_weights_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T20:34:25.710404Z","iopub.status.idle":"2025-06-03T20:34:25.710691Z","shell.execute_reply.started":"2025-06-03T20:34:25.710578Z","shell.execute_reply":"2025-06-03T20:34:25.710591Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### rpl with energy entropy loss results (RPL new paper)","metadata":{}},{"cell_type":"code","source":"class ContrastLoss(nn.Module, ABC):\n    def __init__(self, engine=None, config=None):\n        super(ContrastLoss, self).__init__()\n        self.engine = engine\n        self.temperature = 0.10\n        self.ignore_idx = 255\n        self.ood_idx = 13\n        self.max_views = 512\n\n    def forward(self, city_proj, city_gt, city_pred, ood_proj, ood_gt, ood_pred):\n        city_gt = torch.nn.functional.interpolate(city_gt.unsqueeze(1).float(), size=city_proj.shape[2:],\n                                                  mode='nearest').squeeze().long()\n\n        ood_gt = torch.nn.functional.interpolate(ood_gt.unsqueeze(1).float(), size=ood_proj.shape[2:],\n                                                 mode='nearest').squeeze().long()\n\n        # normalise the embed results\n        city_proj = torch.nn.functional.normalize(city_proj, p=2, dim=1)\n        ood_proj = torch.nn.functional.normalize(ood_proj, p=2, dim=1)\n\n        # randomly extract embed samples within a batch\n        anchor_embeds, anchor_labels, contrs_embeds, contrs_labels = self.extraction_samples(city_proj, city_gt,\n                                                                                             ood_proj, ood_gt)\n\n        # calculate the CoroCL\n        loss = self.info_nce(anchors_=anchor_embeds, a_labels_=anchor_labels.unsqueeze(1), contras_=contrs_embeds,\n                             c_labels_=contrs_labels.unsqueeze(1)) if anchor_embeds.nelement() > 0 else \\\n            torch.tensor([.0], device=city_proj.device)\n\n        return loss\n\n    # The implementation of cross-image contrastive learning is based on:\n    # https://github.com/tfzhou/ContrastiveSeg/blob/287e5d3069ce6d7a1517ddf98e004c00f23f8f99/lib/loss/loss_contrast.py\n    def info_nce(self, anchors_, a_labels_, contras_, c_labels_):\n        # calculates the binary mask: same category => 1, different categories => 0\n        mask = torch.eq(a_labels_, torch.transpose(c_labels_, 0, 1)).float()\n\n        # calculates the dot product\n        anchor_dot_contrast = torch.div(torch.matmul(anchors_, torch.transpose(contras_, 0, 1)),\n                                        self.temperature)\n\n        # for numerical stability\n        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n        logits = anchor_dot_contrast - logits_max.detach()\n\n        # calculates the negative mask\n        neg_mask = 1 - mask\n        \n        # avoid the self duplicate issue\n        mask = mask.fill_diagonal_(0.)\n\n        # sum the negative odot results\n        neg_logits = torch.exp(logits) * neg_mask\n        neg_logits = neg_logits.sum(1, keepdim=True)\n\n        exp_logits = torch.exp(logits)\n\n        # log_prob -> log(exp(x))-log(exp(x) + exp(y))\n        # log_prob -> log{exp(x)/[exp(x)+exp(y)]}\n        log_prob = logits - torch.log(exp_logits + neg_logits)\n\n        # calculate the info-nce based on the positive samples (under same categories)\n        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n        return - mean_log_prob_pos.mean()\n\n    def extraction_samples(self, city_embd, city_label, ood_embd, ood_label):\n        \n        #checking the correct shape\n        if city_label.dim() == 2:\n            city_label = city_label.unsqueeze(0)\n        if ood_label.dim() == 2:\n            ood_label = ood_label.unsqueeze(0)\n\n        # reformat the matrix\n        city_embd = city_embd.flatten(start_dim=2).permute(0, 2, 1)\n        city_label = city_label.flatten(start_dim=1)\n        ood_embd = ood_embd.flatten(start_dim=2).permute(0, 2, 1)\n        ood_label = ood_label.flatten(start_dim=1)\n\n        # define different types of embeds\n        city_positive = city_embd[city_label == self.ood_idx]\n        city_negative = city_embd[(city_label != self.ood_idx) & (city_label != self.ignore_idx)]\n        ood_positive = ood_embd[ood_label == self.ood_idx]\n        ood_negative = ood_embd[(ood_label != self.ood_idx) & (ood_label != self.ignore_idx)]\n\n        # define the number of choice\n        sample_num = int(min(self.max_views, city_positive.shape[0], ood_positive.shape[0],\n                             city_negative.shape[0], ood_negative.shape[0]))\n\n        # randomly extract the anchor set with {city_ood, city_inlier}\n        city_positive_anchor = city_positive[torch.randperm(city_positive.shape[0])][:sample_num]\n        city_negative_anchor = city_negative[torch.randperm(city_negative.shape[0])][:sample_num]\n\n        anchor_embed = torch.cat([city_positive_anchor, city_negative_anchor], dim=0)\n\n        anchor_label = torch.cat([torch.empty(city_positive_anchor.shape[0],\n                                              device=city_positive_anchor.device).fill_(1.),\n                                  torch.empty(city_negative_anchor.shape[0],\n                                              device=city_negative_anchor.device).fill_(0.)])\n\n        # randomly extract the contras set with {city_ood, city_inlier, coco_ood, coco_inlier}\n        city_positive_contras = city_positive_anchor.clone()\n        city_negative_contras = city_negative_anchor.clone()\n        ood_positive_contras = ood_positive[torch.randperm(ood_positive.shape[0])][:sample_num]\n        ood_negative_contras = ood_negative[torch.randperm(ood_negative.shape[0])][:sample_num]\n\n        contrs_embed = torch.cat([city_positive_contras, city_negative_contras,\n                                  ood_positive_contras, ood_negative_contras], dim=0)\n\n        contrs_label = torch.cat([torch.empty(city_positive_contras.shape[0],\n                                              device=city_positive_contras.device).fill_(1.),\n                                  torch.empty(city_negative_contras.shape[0],\n                                              device=city_negative_contras.device).fill_(0.),\n                                  torch.empty(ood_positive_contras.shape[0],\n                                              device=ood_positive_contras.device).fill_(1.),\n                                  torch.empty(ood_negative_contras.shape[0],\n                                              device=ood_negative_contras.device).fill_(0.)])\n\n        return anchor_embed, anchor_label, contrs_embed, contrs_label","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T20:34:25.711515Z","iopub.status.idle":"2025-06-03T20:34:25.711708Z","shell.execute_reply.started":"2025-06-03T20:34:25.711617Z","shell.execute_reply":"2025-06-03T20:34:25.711625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RPLCLDeepLab(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        \n        self.encoder = self.copy_un_freeze_params(model.encoder, unfreeze=False)\n        self.decoder = self.copy_un_freeze_params(model.decoder, unfreeze=False)\n        self.final = nn.Sequential(\n            self.copy_un_freeze_params(model.decoder.block2, unfreeze=False),\n            self.copy_un_freeze_params(model.segmentation_head, unfreeze=False),    \n        )\n        \n        self.atten_aspp_final = nn.Conv2d(256, 304, kernel_size=1, bias=False)\n        \n        self.projection_head = nn.Sequential(\n            nn.Conv2d(256, 304, kernel_size=1)\n        )\n        \n        self.residual_anomaly_block = nn.Sequential(\n            self.copy_un_freeze_params(model.decoder.aspp, unfreeze=True),\n            self.copy_un_freeze_params(model.decoder.up, unfreeze=True),\n        )\n\n\n    def copy_un_freeze_params(self, layer: nn.Module, unfreeze: bool=True) -> nn.Module:\n        \"\"\"\n        function that create a deepcopy of a layer and unfreeze its parameters if unfreeze is True, otherwise freeze it\n\n        return: deepcopy of the layer freezed or unfreezed\n        \"\"\"\n        layer_copy = deepcopy(layer)\n        for param in layer_copy.parameters():\n            param.requires_grad = unfreeze\n        return layer_copy\n\n    def forward(self, x):\n\n        features = self.encoder(x)\n        aspp_features = self.decoder.aspp(features[-1])\n        aspp_features = self.decoder.up(aspp_features)\n        high_res_features = self.decoder.block1(features[2])\n        concat_features = torch.cat([aspp_features, high_res_features], dim=1)\n        \n        res = self.residual_anomaly_block(features[-1])\n\n        res1 = self.atten_aspp_final(res)\n        proj = self.projection_head(res)\n\n        out1 = self.final(concat_features)\n        out2 = self.final(concat_features + res1)\n\n        return out1, out2, proj","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T20:34:25.713562Z","iopub.status.idle":"2025-06-03T20:34:25.713780Z","shell.execute_reply.started":"2025-06-03T20:34:25.713684Z","shell.execute_reply":"2025-06-03T20:34:25.713693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"''''rpl_cr_trainer = Trainer(\n    model= rplcl,\n    train_loader= mix_train_dl,\n    val_loader= mix_val_dl ,\n    loss = energy_loss,\n    contrastive_loss = ContrastLoss(),\n    device= DEVICE,\n    num_classes = len(COLORS),\n    model_name = model_name,\n    cfg= rpl_cfg,\n    #resume_ckpt = torch.load(resume_ckpt)\n)\n\n#rpl_cr_trainer.train()''''","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"scrolled":true,"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T20:34:25.714345Z","iopub.status.idle":"2025-06-03T20:34:25.714578Z","shell.execute_reply.started":"2025-06-03T20:34:25.714483Z","shell.execute_reply":"2025-06-03T20:34:25.714492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}