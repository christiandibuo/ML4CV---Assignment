{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10068624,"sourceType":"datasetVersion","datasetId":6205623}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Author\n0001128790 - Christian Di Buò - christian.dibuo@studio.unibo.it","metadata":{}},{"cell_type":"markdown","source":"### Importing images","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom PIL import Image\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nimport torchvision\nfrom torchvision import transforms\n\nfrom enum import IntEnum\nfrom typing import Optional\nimport subprocess, sys","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"package_name = \"evaluate\"\n\ntry:\n    __import__(package_name)\n    print('already installed')\nexcept ImportError:\n    print(f\"{package_name} is NOT installed! Installing now...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name]);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate\n\n\"\"\"\nSource: https://github.com/hendrycks/anomaly-seg/issues/15#issuecomment-890300278\n\"\"\"\nCOLORS = np.array([\n    [  0,   0,   0],  # unlabeled    =   0,\n    [ 70,  70,  70],  # building     =   1,\n    [190, 153, 153],  # fence        =   2, \n    [250, 170, 160],  # other        =   3,\n    [220,  20,  60],  # pedestrian   =   4, \n    [153, 153, 153],  # pole         =   5,\n    [157, 234,  50],  # road line    =   6, \n    [128,  64, 128],  # road         =   7,\n    [244,  35, 232],  # sidewalk     =   8,\n    [107, 142,  35],  # vegetation   =   9, \n    [  0,   0, 142],  # car          =  10,\n    [102, 102, 156],  # wall         =  11, \n    [220, 220,   0],  # traffic sign =  12,\n    [ 60, 250, 240],  # anomaly      =  13,\n]) \n\n\ndef color(img_pil: str, colors: np.ndarray) -> Image.Image:\n    #img_pil = Image.open(annot_path)\n    img_np = np.array(img_pil)\n    img_new = np.zeros((720, 1280, 3))\n\n    for index, color in enumerate(colors):\n        img_new[img_np == index + 1] = color\n    \n    return Image.fromarray(img_new.astype(\"uint8\"), \"RGB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class StreetHazardsDataset(Dataset):\n    def __init__(self, odgt_file, transform1=None, transform2=None):\n        \"\"\"\n        Args:\n            odgt_file (str): Path to the .odgt file (train, val, or test).\n            transform (callable, optional): Transformations to apply to images and masks.\n        \"\"\"\n\n        self.transform1 = transform1\n        self.transform2 = transform2\n\n        # Load the .odgt file\n        with open(odgt_file, \"r\") as f:\n            odgt_data = json.load(f)\n\n        self.paths = [\n            {\n                \"image\": os.path.join(Path(odgt_file).parent, data[\"fpath_img\"]),\n                \"annotation\": os.path.join(Path(odgt_file).parent, data[\"fpath_segm\"]),\n            }\n            for data in odgt_data \n        ]\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, idx):\n\n        # Build full paths for image and mask\n        image = Image.open(self.paths[idx][\"image\"]).convert(\"RGB\")\n        annotation = Image.open(self.paths[idx][\"annotation\"])\n\n        if self.transform1:\n            image = self.transform1(image)\n            annotation = torch.as_tensor(transforms.functional.pil_to_tensor(annotation), dtype=torch.int64) - 1 # Make class indexes start from 0\n            annotation = self.transform2(annotation).squeeze(0)\n\n        return {\"pixel_values\": image, \"labels\": annotation}\n\n\ndef visualize_annotation(annotation_img: np.ndarray|torch.Tensor, ax=None):\n    \"\"\"\n    Adapted from https://github.com/CVLAB-Unibo/ml4cv-assignment/blob/master/utils/visualize.py\n    \"\"\"\n    if ax is None: ax = plt.gca()\n    annotation_img = np.asarray(annotation_img)\n    img_new = np.zeros((*annotation_img.shape, 3))\n\n    for index, color in enumerate(COLORS):\n        img_new[annotation_img == index] = color\n\n    ax.imshow(img_new / 255.0)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\ndef visualize_scene(image: np.ndarray|torch.Tensor, ax=None):\n    if ax is None: ax = plt.gca()\n    image = np.asarray(image)\n    ax.imshow(np.moveaxis(image, 0, -1))\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Define transforms\ntransform1 = transforms.Compose([\n    transforms.Resize((520, 520), transforms.InterpolationMode.NEAREST),\n    transforms.ToTensor()\n])\n\ntransform2 = transforms.Compose([\n    transforms.Resize((520, 520), transforms.InterpolationMode.NEAREST)\n])\n\n# Create dataset instance\ntrain_dataset = StreetHazardsDataset(\n    odgt_file=\"/kaggle/input/streethazards_train/train/train.odgt\",\n    transform1=transform1,\n    transform2=transform2\n)\nval_dataset = StreetHazardsDataset(\n    odgt_file=\"/kaggle/input/streethazards_train/train/validation.odgt\",\n    transform1=transform1,\n    transform2=transform2\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(val_dataset[0]['labels'].shape, len(val_dataset))\nvisualize_annotation(val_dataset[0]['labels'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import MobileViTImageProcessor, MobileViTForSemanticSegmentation\n\nnum_classes = len(COLORS) - 1\n\n#model_name = \"apple/deeplabv3-mobilevit-xx-small\"\nmodel_name = \"apple/deeplabv3-mobilevit-small\"\nfeature_extractor = MobileViTImageProcessor.from_pretrained(model_name)\nmodel = MobileViTForSemanticSegmentation.from_pretrained(model_name, num_labels=num_classes, ignore_mismatched_sizes=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nepochs = 20\nbatch_size = 8\n\ntraining_args = TrainingArguments(\n    output_dir=\"test_dir\",\n    save_total_limit=1,\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=epochs,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    report_to=\"none\",\n    greater_is_better=True,\n    metric_for_best_model=\"eval_mean_iou\",\n    seed=42,\n    logging_strategy='epoch'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\n# Load mean_iou metric\nmetric = evaluate.load(\"mean_iou\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    \n    # Convert logits and labels to PyTorch tensors if they are NumPy arrays\n    logits = torch.tensor(logits)\n    labels = torch.tensor(labels)\n\n    # Resize logits to match the label size (520x520)\n    logits_resized = F.interpolate(logits, size=labels.shape[1:], mode='bilinear', align_corners=False)\n    \n    # Get the predicted class (argmax over the class dimension)\n    pred = logits_resized.argmax(dim=1)  # Predicted class labels\n    \n    # Convert predictions and labels to numpy for the metric\n    pred = pred.numpy()\n    labels = labels.numpy()\n    \n    # Compute mean IoU\n    result = metric.compute(predictions=pred, references=labels, num_labels= num_classes, ignore_index= 255)\n    \n    return {\"eval_mean_iou\": result[\"mean_iou\"]}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred = trainer.predict(val_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_metrics = compute_metrics((pred.predictions, pred.label_ids))\nprint(test_metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}